{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8653ae218eee7ff0a061f40494ec051b",
     "grade": false,
     "grade_id": "cell-10cefc7b10915ffa",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1 align=\"center\"> Components of Machine Learning.</h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e60031e620ed93776fae6723d2ee6ec8",
     "grade": false,
     "grade_id": "cell-dc391cee3b8d50a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-danger\">\n",
    "   <h3 align=\"center\"> <b>NOTE: This notebook is not graded. Do not submit it.</b> </h3>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f02d4af7aa2d18f2139b447cd778878",
     "grade": false,
     "grade_id": "Introduction",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Machine learning (ML) studies methods that enable a computer to learn from data without explicitly telling it (programming) what to do. Technically, ML methods fit models to data to be able to make accurate predictions and/or inferences about phenomena such as the weather or the behavior of humans. You might use ML applications in you everyday life without even noticing it. Recommendation systems, spam filtering, voice recognition, and chatbots are examples of such applications. Some of us might have our current partner or job (partly) chosen by an ML algorithm used in recommender systems in social networks. In a nutshell, ML tries to \"find a hypothesis that allows predicting a quantity of interest for any data point\". This notebook aims at explaining this informal statement by discussing the three main components of ML: data, hypothesis space, and loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0f7c0e52547a979d487d108913f8c01",
     "grade": false,
     "grade_id": "cell-7a9e65df3c728c17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "After completing this notebook, you\n",
    "\n",
    "will be familiar with following topics:\n",
    "\n",
    "- Three components of machine learning: data, hypothesis space (model) and loss functions. \n",
    "- Properties of data points: features (low-level measurements) and labels (represent high-level facts).\n",
    "- Difference between regression and classification problems based on the nature of the labels.\n",
    "- Validation and test errors as indicators for the performance of a hypothesis outside the training set. \n",
    "\n",
    "will be able to:\n",
    "\n",
    "- find useful definitions for data points, their features, and labels for your \"real-life applications\".\n",
    "- train a linear classifier by minimizing the average logistic loss (logistic regression).\n",
    "- use validation and test sets to compute validation and test errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23883a0ea3c2f26e85692a2bb34d8c39",
     "grade": false,
     "grade_id": "cell-c56fbecdcca9af17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Component - Data\n",
    "\n",
    "ML methods view data as collections of atomic units of information called **data points**. A data point can represent very different things or objects. Data points might represent different days, different countries, different persons, or different planets. The concept of data points is very abstract and, in turn, highly flexible. However, it is important to clearly define the meaning of data points when starting to develop ML applications. \n",
    "Data points are defined by their properties which we roughly divide into two fundamentally different groups referred to as **features** and **labels**.  \n",
    "\n",
    "**Features.** Features are properties of a data point that can be measured or computed in an automated fashion without requiring extensive human supervision. For data points representing smartphone snapshots, a natural choice for the features is the red, green, and blue intensities of each pixel in the snapshot. Each data point is characterized by several (typically a lot of) features, which we will stack into a **numeric array**. The simplest forms of such arrays are either vectors or matrices. However, it will be convenient to allow features of a data point to form numeric arrays of arbitrary (but finite) dimensions.  \n",
    "\n",
    "**Labels.** Besides features, data points are also characterized by higher-level properties which we refer to as **labels**. The labels of a datapoint typically represent some higher-level fact or quantity of interest for that data point. In contrast to features, the label of a data point can often only be determined by a human expert. Consider a data point representing a smartphone snapshot. We could then define the label of this datapoint as $y=1$ if the snapshot contains a cat or $y=0$ if the snapshot does not contain a cat. Depending on the type of label values we distinguish between different ML problems.  \n",
    "\n",
    "**Regression Problems.** We speak of regression problems when data points have numeric labels which are typically represented by a real-number $y \\in \\mathbb{R}$. Having numeric label values allows comparing the quality of different predictions. Consider a data point with the true label value $y=10$ and two predicted label values $\\hat{y}^{(1)}=20$ and $\\hat{y}^{(2)} = 100$ obtained from two different ML methods. Then we can say that the prediction $\\hat{y}^{(1)}$ is better than prediction $\\hat{y}^{(2)}$ since it is closer to the true label value $y=10$.  \n",
    "\n",
    "**Classification Problems.** In classification problems there is only a finite number of different label values. The label of a data point typically indicates the category or class to which that data point belongs to. The most simple setting is **binary classification** where data points belong to exactly one out of two different classes. Here, the label values take on values from a set that contains two elements (e.g., $\\{0,1\\}$ or $\\{-1,1\\}$ or $\\{\\mbox{shows cat},\\mbox{shows no cat}\\}$. If data points belong to exactly one out of more than two categories we speak of the **multiclass classification** problem (image categories \"no cat shown' vs 'one cat shown' and \"more than one cat shown\"). If there are $K$ different categories we might use the label values $\\{1,2,\\ldots, K\\}$. There are also applications where data points can belong to several categories simultaneously (image can \"contain cat\" and \"contain dog\" at the same time). **Multilabel classification** methods use several labels $y_{1},y_{2}$,\\ldots, for each data point. The label $y_{j}$ represents the $j$th category and its value is $y_{j}=1$ if the data point belongs to the $j$th category and $y_{j}=0$ if not.\n",
    "\n",
    "\n",
    "ML can *roughly* be divided into **supervised**- and **unsupervised** learning. A supervised ML model uses the labeled data points as examples to learn a predictor function that takes features of a data point as input and outputs a predicted label. A trained model can then be used to predict labels of data points for which the true labels are unknown. Examples of supervised learning:\n",
    "\n",
    "- linear regression\n",
    "- logistic regression\n",
    "- support vector machines\n",
    "- decision trees\n",
    "\n",
    "In contrast to supervised methods, unsupervised methods do not require the data to be labeled and are in general used for problems related to the structure and distribution of the data. Examples of unsupervised ML methods:\n",
    "\n",
    "- clustering algorithms, which aim to identify different clusters of data points in the dataset\n",
    "- generative models that are used to generate data (see this [example](https://www.youtube.com/watch?v=kSLJriaOumA)).\n",
    "- dimensionality reduction (PCA, t-SNE) that is used for visualization of high dimensional data or as a pre-processing step before other ML methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31920b97e0b316637b8bc78b69e62de5",
     "grade": false,
     "grade_id": "cell-6fa2dc2cb78c4961",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Component - Hypothesis Space (\"Model\")\n",
    "\n",
    "When applying an ML model on labeled data, we want the model to learn a predictor function $h(\\mathbf{x})$ that takes the features of a data point as input and outputs a predicted label $\\hat{y}$. Ideally, we would like our ML model to be able to learn any possible function so that it can find the one that best represents the relationship between the features and the label. This is, however, impossible in practice, and therefore we have to restrict the set of functions that the ML model can learn. This restricted set of predictor functions is referred to as the **hypothesis space** and denoted $\\mathcal{H}$\n",
    "\n",
    "The choice of which hypothesis space to use in a ML method is often informed by some assumption (or intuition) about the relationship between the features and label of a data point. For example, by selecting the set of linear functions of the form\n",
    "\\begin{equation}\n",
    "    h(x) = w \\cdot x, \\; \\text{ where } w \\in \\mathbb{R}\n",
    "\\end{equation}\n",
    "we are effectively assuming that the relationship between features $x$ and and label $y$ is a linear. It is important to understand that such assumptions can rarely be justified in advance, and it is in practice necessary to experiment with models using different hypothesis spaces to find the one that results in the best predictions.\n",
    "\n",
    "### Weights are Model Parameters\n",
    "\n",
    "Consider the above hypothesis space $\\mathcal{H}$ which is constituted by the linear maps\n",
    "\n",
    "\\begin{equation}\n",
    "    h(x) = w \\cdot x, \\; \\text{ where } w \\in \\mathbb{R}. \n",
    "\\end{equation}\n",
    "The elements of $\\mathcal{H}$ are predictor functions $h$ that take as input $x$ and return a prediction $\\hat{y} = wx$. This hypothesis is an exmample of a paramterized hypothesis space. Each hypothesis is fully determined by the value of the weight $w$. We can denote the hypothesis obtained for a given weight $w$ by $h^{(w)}$. Searching (learning) a good hypothesis out of a parametrized hypothesis space is equivalent to searching (or learning) a weight vector that corresponds to a good hypothesis. Another important family of paramtrized hypothesis spaces are those obtained from ariticial neural networks. \n",
    "\n",
    "We refer to the weight $w$ in the above defintion of linear maps as a **model parameter**. Roughly speaking, model parameters are variables that the learning algorithm tunes during the training process to find the best predictor function in the hypothesis space. Models with larger hypothesis spaces than our tiny example have a larger number of parameters. An extreme example is the [GPT-3](https://en.wikipedia.org/wiki/GPT-3) deep learning model, which has **~175 billion model parameters**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cdccd0de306343408e0b8a3d8e96c32",
     "grade": false,
     "grade_id": "cell-1ed770e7af6187b0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3. Component - Loss\n",
    "\n",
    "Loosely speaking, ML methods are optimization or search algorithms that aim at finding (learning) the best hypothesis out of hypothesis space. However, finding the best hypothesis requires a measure of \"success\" of the \"quality\" of a specific hypothesis $h$. ML methods use loss functions to obtain such a quality measure.\n",
    "\n",
    "A loss functions is a rule (or recipe) for quantifying the discrepancy between the predicted label $\\hat{y}=h(\\mathbf{x})$ and the true label $y$ of a data point with features $\\mathbf{x}$. Formally, a loss function is a map that assigns each pair of data point and hypothesis some (non-negative) number $\\mathcal{L}\\big( \\big(\\mathbf{x},y\\big),h)$.  In general, ML methods used loss functions that deliver smaller values if the predicted label is \"closer\" to the true label. However, the precise meaning of \"closer\" depends the choice for the possible label values (which are real-nubmers in regression problems but might be aribtrarily structured sets for classification problems). \n",
    "\n",
    "**Loss Functions for Regression.** The above (rather abstract and) generic definition of a loss function is best understood by looking at specific examples for loss functions. If the label values are numeric, two widely used loss functions are the **squared error loss** $\\mathcal{L}\\big(\\big(\\mathbf{x},y\\big),h \\big) = \\big( y - h(\\mathbf{x}) \\big)^{2}$ and the **absolute error loss** $\\mathcal{L}\\big( \\big(\\mathbf{x},y\\big),h \\big) =| y - h{\\mathbf(x})|.$ \n",
    "\n",
    "**Loss Functions for Classification.** One important criterion when choosing loss functions is if labels are numeric or categorical. In general, loss functions that are suitable for assessing predictions of numeric labels are not a good choice for assessing predictions of label values that represent categories. For binary classification problems, where the labels take on only two different values, we could use the **$0/1$ loss**   \n",
    "\n",
    "\\begin{equation} \n",
    "\\mathcal{L}\\big( \\big(\\mathbf{x},y\\big),h \\big)  = \\begin{cases} 1 & \\mbox{ , if } y = h(\\mathbf{x}) \\\\ & 0  \\mbox{ otherwise.}\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "You can read more about loss functions for classification problems in Chapter 2 of the [MLBook](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf). \n",
    "\n",
    "**Ordinal Label Values.** Some ML applications involve data points with ordinal label values. These label values are somewhat in between numeric (regression) and categorical (classification). Similar to categorical label values, ordinal label values take on values from a finite set. Moreover, similar to numeric label values, ordinal label values have an order. As an example consider data points representing contries and their label being an indicator 0,...,4 of the Covid-19 incidence level. There are loss functions that are particulary suited for assessing predictions of ordinal lavel values (read more in Chapter 2 of the [MLBook](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf)). \n",
    "\n",
    "\n",
    "**Training Error is Average Loss on Training Set.** For many loss functions, we can only evaluate the loss $\\mathcal{L}(\\hat{y}, y)$ incurred by the prediction $\\hat{y}=h(\\mathbf{x})$ if we know the true label value $y$ of the data point. Thus, Most ML methods need to be fed with a set of labeled datapoints $\\mathcal{D} = \\big\\{ \\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big) \\big\\}$. The dataset $\\mathcal{D}$ is referred to as being labeled since it contains datapoints for which we know the true label values. These \"labeled\" data points are then used as a training set by ML method in the following sense. ML methods learn a predictor map $h \\in \\mathcal{H}$ that incurs minimal **average loss** on the training data, \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{E}\\big(h|\\mathcal{D} \\big) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathcal{L}\\big(\\underbrace{\\hat{y}^{(i)}}_{h\\big(\\mathbf{x}^{(i)}\\big)}, y^{(i)} \\big)\n",
    "\\end{equation}\n",
    "\n",
    "The average loss $\\mathcal{E}\\big(h|\\mathcal{D} \\big)$ is the average of the individual losses incurred by using the predictor $h$ to predict the labels of the individual data points in the training set. We sometimes refer to the average loss of a predictor on the training set as the **training error**. \n",
    "\n",
    "**Design Choice.** The loss function used by a ML method is a design choice that must balance between different design constraints. These constraints might arise from limited computational resources (storage, number of GPUs, maximum processing time), statistical properties of data (presence of outliers) and their interpretability. One example for computationally appealing loss functions is the squared error loss since it allows to use simple gradient-based methods for learning a hypothesis. One example for a loss function that results in robustness against outliers is the absolute error loss. One example for an interpretable loss function is the $0/1$ loss which resutts in an \"error-rate\", i.e., the fraction of wrongly classified data points.\n",
    "\n",
    "**Loss, Score and Metric.** Unfortunately, there is rarely a single loss function that satifies all design contraints. Loss functions that are easy to minimize (e.g. using gradient descent) might not be robust against outliers or difficult to interpret. Therefore, it might be useful to use different loss functions within the same ML method. One loss function that is easy to minimize is used for learning a good hypothsis map. Another loss function that is easier to interpret is then used for the final performance avaluation of the learnt hypothesis on a test set. The latter loss function, that is used for the final performance evaluation, is often referred to as \"metric\" or \"score\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a96e9e8330c71dc41cab5f8e64d7f92c",
     "grade": false,
     "grade_id": "cell-eb3cf375e84eb338",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='1.1'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Notation</b></h3>\n",
    "\n",
    "The symbol $\\{ \\}$ indicates a set. The set is a collection of elements, e.g. $\\{1,2,3\\}$ is a set of three numbers and $\\mathcal{D} = \\big\\{ \\big(\\mathbf{x}^{(1)},y^{(1)}\\big),\\ldots,\\big(\\mathbf{x}^{(m)},y^{(m)}\\big) \\big\\}$ is a set of $m$ data points.\n",
    "\n",
    "\\\n",
    "Expression $ \\mathcal{E}\\big(h|\\mathcal{D} \\big)$ denotes the empirical risk or average loss incurred by hypothesis $h$ on the data points in the set $\\mathcal{D}$.\n",
    "\n",
    "More about sets:\n",
    "    \n",
    "- [Introduction to sets](https://www.mathsisfun.com/sets/sets-introduction.html) from mathsisfun.com\n",
    "- [Set symbols](https://www.mathsisfun.com/sets/symbols.html) from mathsisfun.com\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c24a49f077862e3ba4fb737ab6079e0",
     "grade": false,
     "grade_id": "cell-49a371782a3f71e6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Putting Together the Pieces. \n",
    "\n",
    "Now that we have discussed the three main components of ML, let us discuss two particular ML methods that use different choices for data representation and loss function but the same hypothesis space. These two methods are referred to as **linear (least-squares) regression** and **logistic regression**. Both methods consider data points  characterized by a feature vector $\\mathbf{x} = \\big(x_{1},\\ldots,x_{n} \\big)$ consisting of numeric features $x_{1},\\ldots,x_{n}$ of a data point. Moreoever, both methods use the same hypothesis space which is constituted by all linear maps $h(\\mathbf{x}) = \\sum_{j} x_{j} w_{j}$ with (tunable) weights $w_{1},\\ldots,w_{n} \\in \\mathbb{R}$.\n",
    "\n",
    "\n",
    "### Linear (Least-Squares) Regression\n",
    "\n",
    "For regression problems, where data points have numeric labels, a widely used choice for the loss function is the **squared error loss** $(y - \\hat{y})^{2}$. The average squared error loss incurred on a training set is referred to as the **mean squared error** (MSE):\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^{2} = \\frac{1}{m} \\sum_{i=1}^{m} \\big(y^{(i)} - \\underbrace{h\\big(x^{(i)}\\big)}_{\\mbox{predicted label } \\hat{y}^{(i)}} \\big)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "### Logistic Regression \n",
    "\n",
    "We now consider ML applications that involve data points with a binary label. Thus, the label of each data point is  one out of two possible values. Without loss of generality we assume these two possible label values are $0$ and $1$. (You can easily map transform label values for binary classification problems using `LabelEncoder()`)) The squared error loss is a bad choice for the loss function for classification problems where labels represent class memberships and have no numeric meaning. Consider a binary classification problem where data points represent a webcam snapshot of an animal. Here, we might define the label of a data point as $y=0$ if it shows a cat and $y=1$ if the snapshot does not show any cat. We can use ML methods to learn a hypothesis $h(\\mathbf{x}) \\in [0,1]$ whose value is the (estimated) probability that the label value is $1$. If the estimated probability is less than $1/2$, we define the predicted label as $\\hat{y}=0$, if the probability is at least $1/2$, the predicted label is $\\hat{y}=1$. We can denote this classification rule as \n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{y} = \\begin{cases} 1 & \\mbox{ for }h(\\mathbf{x}) \\ \\geq 0.5 \\\\ 0 & \\mbox{ for } h(\\mathbf{x}) < 0.5 \\end{cases}\n",
    " \\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "We use the value $h(\\mathbf{x})$ to construct an estimate for the probability $p(y=1)$ via $p(y=1) = \\frac{1}{1+e^{-h(\\mathbf{x})}}$. To learn a useful hypothesis $h(\\mathbf{x})$ we minimize the logistic (or cross-entropy) loss \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}(y,h(\\cdot)) := -y\\ln\\big(p(y=1)\\big)-(1-y)\\ln\\big(p(y=0)\\big).\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"R0_data/logreg.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef811d6d26618cc367be6e25499a7648",
     "grade": false,
     "grade_id": "cell-623263907fe58df3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Logistic regression learns the weights of a linear classifier $h(\\mathbf{x})$ by minimizing the average **logistic loss**:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    (1/m)\\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(p(y=1)\\big)-(1-y^{(i)})\\ln\\big(p(y=0)\\big) \\big] \\\\ = (1/m)    \\sum_{i=1}^{m}\\big[ -y^{(i)}\\ln\\big(\\sigma(w \\cdot x)\\big)-(1-y^{(i)})\\ln\\big(1-\\sigma(w \\cdot x)\\big) \\big]\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The average logistic loss is evaluated for a set of data points (the training set) with known label $y^{(i)}$. \n",
    "\n",
    "If you are interested in logistic regression & logistic loss details, you can find more explanations in the video tutorials: \n",
    "- [Andrew Ng, ML course Lecture 6.1](https://www.youtube.com/watch?v=-la3q9d7AKQ) \n",
    "- [Andrew Ng, ML course Lecture 6.2](https://www.youtube.com/watch?v=t1IT5hZfS48)\n",
    "- [StatQuest: Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c86787fcbab0aa85fd626476707575f5",
     "grade": false,
     "grade_id": "cell-20c5caf5ed2f29ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='1.1'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Notation</b></h3>\n",
    "\n",
    "The symbol $\\sum$ (uppercase greek letter sigma) indicates a sum. The sum of all elements in the list $(x_1, x_2, \\ldots, x_n)$ (e.g., forming a vector $\\mathbf{x}$) is denoted by\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^{n}{x}_{i} = {x_1}+...+{x_n}\n",
    "\\end{equation}\n",
    "\n",
    "We can then express an average $\\frac{1}{n}({x_1}+...+{x_n})$ of $n$ numbers conveniently as\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{1}{n}\\sum_{i=1}^{n}{x}_{i} =(1/n) \\big( {x_1}+...+{x_n} \\big) \n",
    "\\end{equation}\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7048c9dcb88f939f06d336baf961332b",
     "grade": false,
     "grade_id": "cell-b49d7c0697704b6d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The Machine Learning Pipeline\n",
    "\n",
    "A typical ML workflow is as follows: \n",
    "\n",
    "\\\n",
    "<img src=\"R0_data/MLsteps2.png\" width=700>\n",
    "\n",
    "- analyze data:\\\n",
    "    dataset quality has a huge impact on ML methods performance, as the whole idea of ML is to learn from data. You will not be able to solve ML problems with low-quality data. As you may have heard saying - \"garbage in, garbage out\". What is bad data and how do we check the quality of a dataset? There are few typical problems with data:\n",
    "    \n",
    "    * small dataset\n",
    "    * missing values\n",
    "    * high noise\n",
    "    * biased sampling procedure \\\n",
    "    (for more details see pp.23-27 of \"Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron)\n",
    "    \n",
    "\n",
    "- choose hypothesis space (model):\n",
    "\n",
    "   in addition to good quality data, an appropriate model must be chosen. For example, for a regression problem, you can choose linear regression (hypothesis space of linear predictors) or decision tree (hypothesis space of piecewise functions).  \n",
    "   <img src=\"R0_data/hyp1.png\"> \n",
    "   \n",
    "   \n",
    "- choose a loss function:\n",
    "\n",
    "  Although machine learning engineers can choose or even invent new loss functions, there are some popular ones that are used the majority of the time:\n",
    "  - mean square error for regression problems\n",
    "  - logistic loss for classification problems\n",
    "    \n",
    "    \n",
    "- Find the best predictor (model):\n",
    "  \n",
    "  given a hypothesis space and a loss function, we need to find the best model within this hypothesis space. The best model means a model with the lowest loss function value. There are many different algorithms for finding optimal predictors (model). Iterative algorithms, such as gradient descent, perform the same step many times in order to find the optimal solution. Sometimes, we can find the best predictor with just one step. For example, there is a so-called 'closed-form solution' for linear regression and it is possible to find the best parameters for linear predictors by using [normal equation](https://www.youtube.com/watch?v=NN7mBupK-8o)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4da2c7fd1a006554235afc798ffff18b",
     "grade": false,
     "grade_id": "cell-f25ec721c0784f94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Generalization and Validation\n",
    "\n",
    "The basic workflow of ML seems quite simple: get some data to form a training set, choose a hypothesis space and loss function and then solve an optimization problem to get the hypothesis with smallest training error. Are we done? The answer is a resounding NO! The goal of ML is to find a hypothesis that yields good predictions for **any** data point and not just those in the training set. We must ensure that a hypothesis that has small (or minimum) training error **generalizes** well to data points outside to training data. \n",
    "\n",
    "Validation is a simple but powerful technique to probe a learnt hypothesis (that minimizes training erro) outside the training set. The idea is to split the available data points into two subsets, a training set and a validation set. We illustrate this split in the plots below by colourint training set blue and validation set orange. \n",
    "The left plot shows a linear hypothesis (predictor) $h(x)=w_{1}+w_{2}x$ is fitted to training dataset (blue dots). This is the so-called training set. Orange dots are \"new\" samples, data that the model didn't see during the fitting process. This set is called a test set. \n",
    " \n",
    "It seems that there is a non-linear relationship between features and labels of data points. Therefore, there is no good linear hypothesis map for approximating the relationship between features and labels which means that the training error is large. We might say that the linear model underfits the training data. \n",
    "\n",
    "The plot in the middle shows polynomial function of degree $3$: $h(x)=w_{1}+w_{2}x+w_3{x}^{2}+w_{4}{x}^{3}$, which seems to fit data quite well. Plot on the right is polynomial function of degree 6: $h(x)=w_{1}+w_{2}x+w_{3}{x}^{2}+...+w_{7}{x}^{6}$. It fits training data very well. In fact, it fits training data too well, as predictions for a few points from the \"new\" dataset are quite bad (two orange points from the right side). This is an example of overfitting when a model has too many parameters.\n",
    "\n",
    "<img src=\"R0_data/hyp2.png\"> \n",
    "\n",
    "If we will plot MSE loss against a model complexity (here degree of polynomial), we will see plot like this:\n",
    "\n",
    "<img src=\"R0_data/complexity.png\" width=600> \n",
    "\n",
    "The blue line shows MSE for the data set used in training and the orange line shows MSE for new, test data, which the model did not see before. Low complexity (degree) models underfit training and test sets. High complexity models fit training data perfectly, while the loss on new data set is skyrocketing (note log scale of the y-axis). In the middle, there is a \"sweet spot\", where both, training and test loss values are relatively low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd2bbed5fb78ed6013438eba46180be0",
     "grade": false,
     "grade_id": "cell-393e3053cee7cb3c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Components of Machine Learning - Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7f7efd7d87a8d981b762708c195fa9a",
     "grade": false,
     "grade_id": "cell-bacd34c03a91bd4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Data</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3653a9920d7278244a8c901808b74780",
     "grade": false,
     "grade_id": "cell-0fa0f89db5ce4ae0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this part, you will learn how to formulate and solve a classification problem. Classification problems arise from data points whose labels have only a finite number of different values. Each of these values represents a particular class of category to which data points can belong. The most simple classification problem is a binary classification problem where the label can take on only two distinct values such as  $y=0$ vs $y=1$ or  $y$=\"picture includes a cat\" vs.$y$=\"picture does not include a cat\". The label $y$ of a data point indicates to which class (or category) the data point belongs.\n",
    "\n",
    "We consider a widely used method for solving classification problems - logistic regression. Logistic regression is a classification algorithm that uses a linear function to classify data points into distinct categories. \n",
    "\n",
    "We will use sklearn [iris dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-plants-dataset) consisting of 150 data points, 3 classes (Setosa, Versicolour, Virginica). Data points are characterized by 4 features: sepal length, sepal width, petal length, petal width, but we will use only the first two, as it is easier to visualize in 2D. We will also restrict ourselves to two classes for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd8c0a5f2c6e9203b6cdcf7fbc31699f",
     "grade": false,
     "grade_id": "cell-9e6330c7ec6492a5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<img src='R0_data/iris.png' width=700>\n",
    "\n",
    "<a href=\"https://www.datacamp.com/community/tutorials/machine-learning-in-r\"><p style=\"text-align:center\">source</p></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip3 install python-utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa0e3dea9883676333d6cb8fd774190f",
     "grade": false,
     "grade_id": "cell-017a9173c85652e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "* {\n",
       "  margin: 0;\n",
       "  padding: 0;\n",
       "  box-sizing: border-box;\n",
       "}\n",
       "\n",
       "*,\n",
       "*:before,\n",
       "*:after {\n",
       "\tbox-sizing: inherit;\n",
       "}\n",
       "\n",
       "body {\n",
       "font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, Helvetica,\n",
       "    Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\";\n",
       "}\n",
       "\n",
       ".title-container {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       ".title {\n",
       "  font-weight: 600;\n",
       "}\n",
       "\n",
       ".subtitle {\n",
       "  margin: 10px 0px;\n",
       "  color: #888888;\n",
       "  font-size: 25px;\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".main-container {\n",
       "  padding: 15px;\n",
       "}\n",
       "\n",
       ".card-container {\n",
       "  display: flex;\n",
       "  flex-wrap: wrap;\n",
       "  justify-content: space-between;\n",
       "}\n",
       "\n",
       ".card {\n",
       "  margin: 20px;\n",
       "  padding: 20px;\n",
       "  width: 100%;\n",
       "  min-height: 200px;\n",
       "  display: grid;\n",
       "  grid-template-columns: 1fr 1fr 1fr;\n",
       "  gap: 10px;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "  transition: all 0.5s;\n",
       "}\n",
       "\n",
       ".card.small {\n",
       "  width: 50%;\n",
       "  min-height: 100px;\n",
       "  grid-template-columns: 1fr 1fr;\n",
       "}\n",
       "\n",
       ".card:hover {\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.4);\n",
       "  transform: scale(1.01);\n",
       "}\n",
       "\n",
       ".card__title {\n",
       "  grid-columnn-start: 1;\n",
       "  grid-columnn-end: -1;\n",
       "  font-weight: 400;\n",
       "  color: #ffffff;\n",
       "}\n",
       "\n",
       ".test-input {\n",
       "  grid-column-start: 1;\n",
       "  grid-column-end: 2;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-output {\n",
       "  grid-column-start: 2;\n",
       "  grid-column-end: 3;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".test-expected-output {\n",
       "  grid-column-start: 3;\n",
       "  grid-column-end: 4;\n",
       "  color: #40413e;\n",
       "}\n",
       "\n",
       ".card-failure {\n",
       "  background: radial-gradient(#fbc1cc, #fa99b2);\n",
       "}\n",
       "\n",
       ".card-failure .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f057\";\n",
       "}\n",
       "\n",
       ".card-success {\n",
       "  background: radial-gradient(#60efbc, #58d5c9);\n",
       "}\n",
       "\n",
       ".card-success .card__title::before {\n",
       "    display: inline-block;\n",
       "    margin-right: 5px;\n",
       "    font-style: normal;\n",
       "    font-variant: normal;\n",
       "    text-rendering: auto;\n",
       "    -webkit-font-smoothing: antialiased;\n",
       "    font-family: \"Font Awesome 5 Free\";\n",
       "    font-weight: 900;\n",
       "    content: \"\\f058\";\n",
       "}\n",
       "\n",
       ".card-info {\n",
       "  background: radial-gradient(#1fe4f5, #3fbafe);\n",
       "}\n",
       "\n",
       "@media (max-width: 1600px) {\n",
       "  .card-container {\n",
       "    justify-content: center;\n",
       "  }\n",
       "}\n",
       "\n",
       ".code-block {\n",
       "  padding: 5px;\n",
       "  background-color: #f3f7f7;\n",
       "  color: black;\n",
       "  border-radius: 10px;\n",
       "  box-shadow: 0px 6px 10px rgba(0, 0, 0, 0.25);\n",
       "}\n",
       "\n",
       "details {\n",
       "\tfont-size: 1rem;\n",
       "\tbox-shadow: 0 10px 15px -5px rgba(0, 0, 0, 0.1),\n",
       "\t\t0 10px 10px -5px rgba(0, 0, 0, 0.04);\n",
       "\twidth: 100%;\n",
       "\tbackground: #ffffff;\n",
       "\tborder-radius: 10px;\n",
       "\tposition: relative;\n",
       "}\n",
       "\n",
       "details:hover {\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".summary-title {\n",
       "    user-select: none;\n",
       "    margin-left: 5px;\n",
       "}\n",
       "\n",
       ".summary-content {\n",
       "    border: 2px solid #0C7B89;\n",
       "    cursor: default;\n",
       "    padding: 1em;\n",
       "    font-weight: 300;\n",
       "    line-height: 1.5;\n",
       "}\n",
       "\n",
       "summary {\n",
       "   color: white;\n",
       "   font-size: large;\n",
       "   font-weight: bold;\n",
       "   padding: 1em;\n",
       "   background-color: #0C7B89;\n",
       "   border-radius: 8px;\n",
       "   list-style: none;\n",
       "}\n",
       "\n",
       "details[open] summary {\n",
       "    border-radius: 8px 8px 0 0;\n",
       "}\n",
       "\n",
       "details[open] summary::before {\n",
       "  transform: rotate(90deg);\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "details summary::before {\n",
       "  position: absolute;\n",
       "  will-change: transform;\n",
       "  transition: transform 300ms ease;\n",
       "  font-family: \"Font Awesome 5 Free\";\n",
       "  color: #fff;\n",
       "  font-size: 1.1rem;\n",
       "  content: \"\\f105\";\n",
       "  left: 0;\n",
       "  display: inline-block;\n",
       "  width: 1.6rem;\n",
       "  text-align: center;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       "summary:focus {\n",
       "  outline: none;\n",
       "}\n",
       "\n",
       "summary::-webkit-details-marker {\n",
       "    display: none;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np                   # the \"numpy\" package provides methods for processing and manipulating numerical arrays\n",
    "import matplotlib.pyplot as plt      # library providing tools for plotting data\n",
    "\n",
    "from utils.styles import load_styles # custom CSS style \n",
    "\n",
    "load_styles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "194fa3bbac58cc3920be09b8ace8db9c",
     "grade": false,
     "grade_id": "cell-c7038049a8817e82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In notebooks you will see yellow sections with student tasks. You need to read instructions and complete a code. The place to put your code is marked as:\n",
    "    \n",
    "`# YOUR CODE HERE`\\\n",
    "`raise NotImplementedError`\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Often you will see comments and pre-filled code lines in cell:\n",
    "    \n",
    "`# import load_iris module` <--- This is a comment\\\n",
    "`# from ... import ...`   <---  This is a code line you need to complete\n",
    "\n",
    "`# load data` <--- This is a comment\\\n",
    "`# data = ... `  <---  This is a code line you need to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7c90c3c481ced88bc02fc0c24b2fef46",
     "grade": false,
     "grade_id": "cell-655143d94c88e80c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student Task.</b> Load Iris dataset.</h3>\n",
    "\n",
    "Your task is to:\n",
    " \n",
    "- import load_iris module from sklearn.datasets\n",
    "- load dataset and store in variable `data`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76b0563045790d190822346cec2f33f",
     "grade": false,
     "grade_id": "cell-2293a04a5837f4f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        <span class=\"summary-title\">Hints. Click to open.</span>\n",
    "    </summary>\n",
    "    <div class=\"summary-content\">\n",
    "This is a hint cell. Sometimes we put extra information about task here. It is adviced to try to solve task without hints first. \n",
    "        <p>\n",
    "            For this task you need to fill in gaps (...) and remove comment tag <code>#</code>:\n",
    "        </p>\n",
    "        <ul>\n",
    "             <li>\n",
    "                 <code># import load_iris module</code>\n",
    "             </li>\n",
    "             <li>\n",
    "                 <code>from sklearn.datasets import load_iris</code>\n",
    "            </li>\n",
    "            <li>\n",
    "                 <code># load data</code>\n",
    "             </li>\n",
    "             <li>\n",
    "                 <code>data = load_iris()</code>\n",
    "            </li>\n",
    "        </ul>\n",
    "      Finally, remove <code>raise NotImplementedError</code> otherwise it will return an error when code cell is run.\n",
    "     </div>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96466c715af3a6ef37ddbd19dbd8e82e",
     "grade": false,
     "grade_id": "cell-0adf687129203d75",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__class__', '__contains__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'count', 'index']\n",
      "\n",
      "Features: (     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                  5.1               3.5                1.4               0.2\n",
      "1                  4.9               3.0                1.4               0.2\n",
      "2                  4.7               3.2                1.3               0.2\n",
      "3                  4.6               3.1                1.5               0.2\n",
      "4                  5.0               3.6                1.4               0.2\n",
      "..                 ...               ...                ...               ...\n",
      "145                6.7               3.0                5.2               2.3\n",
      "146                6.3               2.5                5.0               1.9\n",
      "147                6.5               3.0                5.2               2.0\n",
      "148                6.2               3.4                5.4               2.3\n",
      "149                5.9               3.0                5.1               1.8\n",
      "\n",
      "[150 rows x 4 columns], 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "145    2\n",
      "146    2\n",
      "147    2\n",
      "148    2\n",
      "149    2\n",
      "Name: target, Length: 150, dtype: int64)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# import load_iris module\n",
    "# from ... import ...\n",
    "\n",
    "# load data\n",
    "# data = ...\n",
    "\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# import load_iris module\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "iris_data = load_iris(return_X_y=True, as_frame=True)\n",
    "\n",
    "print(dir(iris_data))\n",
    "# print out features' names\n",
    "#print(\"\\nFeatures:\", data.feature_names)\n",
    "print(\"\\nFeatures:\", iris_data)\n",
    "\n",
    "# print out classes for classification\n",
    "#print(\"\\nClassess:\", data.target_names)\n",
    "\n",
    "fname = 'load_iris'\n",
    "\n",
    "loader = getattr(datasets, fname)()\n",
    "\n",
    "df = pd.DataFrame(loader['data'], columns = loader['feature_names'])\n",
    "\n",
    "df['target'] = loader['target']\n",
    "\n",
    "df.head(2)\n",
    "\n",
    "#print(\"\\nFeature:\", iris_data_frame[\"feature_names\"])\n",
    "\n",
    "# length\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "216b0e1d265349e1ad9d1ff80f0365cf",
     "grade": false,
     "grade_id": "cell-f655ec2117c35191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below you can see Sanity checks. They will check few properties of your solution, to make sure it is not completely irrelevant. \n",
    "\n",
    "\n",
    "<div class=\" alert alert-danger\">\n",
    "    <h3 align=\"center\">Note, that passing sanity checks does not mean that the solution is correct!</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "807f407469cbc99d88252443d88cfd91",
     "grade": false,
     "grade_id": "cell-5fc676e952e7b553",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "length of data should be 7!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b273b5f645f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Sanity checks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"length of data should be 7!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'setosa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"First target name should be 'setosa'!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sanity checks passed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: length of data should be 7!"
     ]
    }
   ],
   "source": [
    "# Sanity checks \n",
    "assert len(data) == 7, \"length of data should be 7!\"\n",
    "assert data.target_names[0]=='setosa', \"First target name should be 'setosa'!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3911f6a4545dc3d02d35c56bdff8b37",
     "grade": false,
     "grade_id": "cell-eb859bdbe90f7fc7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below is a hidden cell. It contains test for student task solution and is hidden from students. The hidden test will be run after deadline. You can see the content of hidden cells after deadline in your html feedback files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "563cb037fc2954862a9819f45f7d1a28",
     "grade": true,
     "grade_id": "cell-3adfcd1b4832af7e",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6cfac23f464c714e33e1a9fba40b982d",
     "grade": false,
     "grade_id": "cell-7b565d693706323f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix dimensions:  (100, 2)\n",
      "Label vector dimensions:  (100,)\n"
     ]
    }
   ],
   "source": [
    "# load data as numpy array\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# choose first 2 features\n",
    "ind = np.where((y==1) | (y==2))[0]\n",
    "y = y[ind]\n",
    "X = X[ind,:2]\n",
    "\n",
    "print(\"Feature matrix dimensions: \", X.shape)\n",
    "print(\"Label vector dimensions: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c94df7c3f6c60784bd93c69036363fe6",
     "grade": false,
     "grade_id": "cell-dd6df77263d4b4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It is a good idea to study your data first. Let's visualize the dataset and see how two classes of iris plants related to each other. We will plot to scatter plot and histograms for two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7cf6fd87a66451d0c35cfc64a1d35b89",
     "grade": false,
     "grade_id": "cell-236204a59aebfa43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-96e222625d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# set seaborn theme for plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# set seaborn theme for plots\n",
    "sns.set_theme()\n",
    "\n",
    "# plot data\n",
    "fig, axes = plt.subplots(1,3, figsize=(15,4))\n",
    "# plot histogram of first feature\n",
    "sns.histplot(X[y==1,0], kde=True, ax=axes[0], color='b').set_title('first feature')\n",
    "sns.histplot(X[y==2,0], kde=True, ax=axes[0], color='r')\n",
    "# plot histogram of second feature\n",
    "sns.histplot(X[y==1,1], kde=True, ax=axes[1], color='b').set_title('second feature')\n",
    "sns.histplot(X[y==2,1], kde=True, ax=axes[1], color='r')\n",
    "\n",
    "# plot data points\n",
    "sns.scatterplot(ax=axes[2], x=X[:,0],y=X[:,1], hue=y, palette=['b','r'], legend=False)\n",
    "\n",
    "plt.xlabel('first feature')\n",
    "plt.ylabel('second feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b7f2a567a567bbfcef81756e22a6fbb5",
     "grade": false,
     "grade_id": "cell-3b012b2ec010d510",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, data distribution is similar to a normal distribution, there are no apparent outliers. Scatterplot and histograms show great overlap in the distribution of features in the two classes. This means that the separation of two classes might be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4523f60abe2e289e820dd78e399d10d",
     "grade": false,
     "grade_id": "cell-c92b40fd6f63063d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Hypothesis space</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1009b4d8da444638e380ca07e642c621",
     "grade": false,
     "grade_id": "cell-0984e28ed06f8411",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define hypothesis space / model\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2305f99aab9dd5d4ba3072d90930830",
     "grade": false,
     "grade_id": "cell-d7dab00cf89bef26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Loss function</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24bdb0e5747015b0ca622192d9da376e",
     "grade": false,
     "grade_id": "cell-483fb2c72174478e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The loss function for logistic regression implemented in sklearn is described here: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7850c69b994d5e7f9beb703bb8e7853a",
     "grade": false,
     "grade_id": "cell-c24bb59a9ddcd4c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Training/ Fitting a model</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0865baff6e007288f3f10983f1d4a8ef",
     "grade": false,
     "grade_id": "cell-e5cbcf9c43c24d83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit logistic regression\n",
    "clf.fit(X, y)\n",
    "# calculate the accuracy of the predictions\n",
    "y_pred = clf.predict(X)\n",
    "accuracy = clf.score(X, y)\n",
    "print(f\"Accuracy of classification: {round(100*accuracy, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb7b45889a1b47a39770f19d9e202c8d",
     "grade": false,
     "grade_id": "cell-07faebe246edf8ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we plot linear decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22b698b2914403a2b20caabb151f5dd0",
     "grade": false,
     "grade_id": "cell-c51a832e9469389b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# get the weights of the fitted model\n",
    "w = clf.coef_ \n",
    "w = w.reshape(-1)\n",
    "\n",
    "# minimum and maximum values of features x1 and x2\n",
    "x1_min, x2_min = np.min(X, axis=0)\n",
    "x1_max, x2_max = np.max(X, axis=0)\n",
    "\n",
    "# plot the decision boundary h(x) = 0\n",
    "# for data with 2 features this means w1x1 + w2x2 + bias = 0 --> x2 = (-1/w2)*(w1x1+bias)\n",
    "x_grid = np.linspace(x1_min, x1_max, 100)\n",
    "y_boundary = (-1/w[1])*(x_grid*w[0] + clf.intercept_)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(5, 4))\n",
    "\n",
    "# plot data points belonging to class 1 and 2\n",
    "sns.scatterplot(ax=axes, x=X[:,0],y=X[:,1], hue=y, palette=['b','r'], s=50, legend=False)\n",
    "# plot decision boundary\n",
    "axes.plot(x_grid, y_boundary, color='green')\n",
    "# display x- and y-axis labels\n",
    "axes.set_xlabel(r'$x_{1}$')\n",
    "axes.set_ylabel(r'$x_{2}$')\n",
    "# display title of figure\n",
    "axes.set_title('Decision boundary', fontsize=16)\n",
    "# set axes limits\n",
    "axes.set_xlim(x1_min-.5, x1_max+.5)\n",
    "axes.set_ylim(x2_min-0.5, x2_max+0.5)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9cb156da576bbebdcc0674da835afa4",
     "grade": false,
     "grade_id": "cell-dd197dab6f08e8a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We have briefly mentioned that when overfitting happens, a model shows good results on the training set (data set used to fit/train a model), but performs poorly on a new dataset. In this case, it is said that a model not generalizing well. **Generalisation** is an ability to perform well on new data. How can we estimate the generalization of a model? As we cannot use loss/score values obtained on the training set, we need additional data set, which will be used only for the final model evaluation. This is the so-called **test set**.\\\n",
    "If you also want to choose between different models (e.g. logistic regression vs decision tree), you will need one more \"new\" dataset - **validation dataset**. The procedure then as follows:\n",
    "\n",
    "- train two models on the training dataset\n",
    "- evaluate two trained models on the validation set\n",
    "- choose the best model\n",
    "- do a final evaluation of chosen model on the test set\n",
    "\n",
    "More about training-validation-test sets:\n",
    "\n",
    "- [What is the Difference Between Test and Validation Datasets? (Machine Learning Mastery blog)](https://machinelearningmastery.com/difference-test-validation-datasets/)\n",
    "- [Training, validation, and test sets (Wikipedia)](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets)\n",
    "- [Cross-validation: evaluating estimator performance (sklearn documentation)](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18dee5a30da8ce48ad1ba757a41a10df",
     "grade": false,
     "grade_id": "cell-8e78bff51ed1e213",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's classify iris plants again, but now split the dataset on training and test sets. Performance score on the test set will show a more realistic estimate of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cdf0c2bf0c06ba01479a42c3c6c95427",
     "grade": false,
     "grade_id": "cell-2b4802e82c7c554d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load data as numpy array\n",
    "X, y = load_iris(return_X_y=True)\n",
    "# choose first 2 features\n",
    "ind = np.where((y==1) | (y==2))[0]\n",
    "y = y[ind]\n",
    "X = X[ind,:2]\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "print(\"Training set dimensions: \", X_train.shape)\n",
    "print(\"Test set dimensions: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "992f7e966e1c6b5fc385ca6d1dda4ba6",
     "grade": false,
     "grade_id": "cell-675582fb16cfe31e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# fit logistic regression\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# calculate the accuracy of the predictions\n",
    "train_accuracy = clf.score(X_train, y_train)\n",
    "test_accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training accuracy of classification: {round(100*train_accuracy, 2)}%\")\n",
    "print(f\"Test accuracy of classification: {round(100*test_accuracy, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4893548041bc663252259d207c74afb5",
     "grade": false,
     "grade_id": "cell-bcae43d777d436ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Note, that accuracy on the test set is lower than on the training set. In addition, accuracy on the training set itself is lower than previously. This might be due to the small size of the dataset, effect which can be alleviated by using [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d5bb2c4b8ff9cc0a055d211a321f81",
     "grade": false,
     "grade_id": "cell-2183fe43035baf82",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# create a logistic regression model\n",
    "clf = LogisticRegression(random_state=0)\n",
    "# data splitting to train-val sets, fitting and evaluation \n",
    "# is performed \"under the hood\" of `cross_val_score()` function.\n",
    "# output scores are accuracies on validation folds\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "\n",
    "print(f\"Cross-validation scores: {scores}%\")\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a657d1ee0f13c7447b6385f28715ac5",
     "grade": false,
     "grade_id": "cell-484c580a00b0f10f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can say that our model on average will perfom with 74±12% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39ab74cd1948948cfebd02ba6ae0f647",
     "grade": false,
     "grade_id": "cell-297557ed901a0fe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-info\">\n",
    "    <h3 align='center'><b>Predictions</b></h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0f91de2bc494db3f3eced4e0fea99962",
     "grade": false,
     "grade_id": "cell-c817cfbfb8339a38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# traing the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# get predictions \n",
    "predict = clf.predict(X)\n",
    "\n",
    "# plot true and predicted labels and decision boundary\n",
    "fig, axes = plt.subplots(1,2, sharex=True, sharey=True,  figsize=(9,3))\n",
    "\n",
    "# plot data points set with true lables\n",
    "sns.scatterplot(ax=axes[0], x=X[:,0],y=X[:,1], hue=y, palette=['b','r'], legend=False)\n",
    "# plot decision boundary\n",
    "axes[0].plot(x_grid, y_boundary, color='green')\n",
    "# plot data points with predicted lables\n",
    "sns.scatterplot(ax=axes[1], x=X[:,0],y=X[:,1], hue=predict, palette=['b','r'], legend=False)\n",
    "# plot decision boundary\n",
    "axes[1].plot(x_grid, y_boundary, color='green')\n",
    "\n",
    "#set axes limits\n",
    "axes[0].set_xlim(x1_min-.5, x1_max+.5)\n",
    "axes[0].set_ylim(x2_min-0.5, x2_max+0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "774d9bc5a00dc6be99f75ed90fa34cea",
     "grade": false,
     "grade_id": "cell-a5294c917c05820c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "More about LogisticRegression class in sklearn:\\\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0ac4ab70dd6173c0a0e5051312bd551",
     "grade": false,
     "grade_id": "cell-b8db26fa053096fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Key takeaways\n",
    "\n",
    "- ML methods can be decomposed into three components: **data**, **hypothesis space** (or **model**), and a **loss function**.\n",
    "\n",
    "- Data(sets) consist of (typically large sets of) individual data points of similar type (e.g. \"persons\", \"flowers\", \"images\", \"random variables\" or \"1 km by 1km areas on the earth surface\").  \n",
    "\n",
    "- Each individual data point is characterized by its features $\\mathbf{x}$ and label(s) $y$. The features can be determined easily (by hard and software) whereas the labels are higher-level facts whose determination needs \"intelligence\" (human or artificial). \n",
    "\n",
    "- ML methods search (or \"learn\") a hypothesis map (or predictor function) $h(\\mathbf{x})$ that allows to estimate/predict/approximate the label $y$ of a data point based solely on its features $\\mathbf{x}$, $y \\approx h(\\mathbf{x})$. \n",
    "\n",
    "- ML methods have only finite computatonal resrouces and therefore can only search a subset of possible maps (there are too many of them!). This subset is the hypothesis space (or model) $\\mathcal{H}$ and consists of the (feasible or allowed) predictor maps that might be learnt by a ML mehtod. \n",
    "\n",
    "- ML methods use different **loss functions** to measure the quality of a prediction $\\hat{y} = h(\\mathbf{x})$ for the true label value $y$. Maybe the most widely-used loss function (for numeric labels) is the squared error loss $(y - \\hat{y})^{2}$. \n",
    "\n",
    "- The goal of an ML method is: \"Find hypothesis out of hypothesis space such that loss incurred by its predictions are minimized for any data point\". "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
