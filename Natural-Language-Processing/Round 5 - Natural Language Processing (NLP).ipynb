{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed238a7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c91faa3be87d36ac753d568057f6c88",
     "grade": false,
     "grade_id": "cell-dcb815f16014454e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# CS-EJ3311 - Deep Learning with Python, 09.09.2021-18.12.2021\n",
    "\n",
    "## Round 5 - Natural Language Processing\n",
    "\n",
    "This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python 13.09.-17.12.2021\\\n",
    "Aalto University (Espoo, Finland)\\\n",
    "fitech.io (Finland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659fd73f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03eb6a267220a5d6901954f0b75cc962",
     "grade": false,
     "grade_id": "cell-5364e6a4492cb764",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you are familiar with virtual assistants like [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), [Google Assistant](https://en.wikipedia.org/wiki/Google_Assistant) or if your email provider automatically classifies your emails into different categories (promotions, updates, forums, spams), or if you use Google Translator, then you have been using Natural Language Processing (NLP). These are not the only places where you can find NLP applications in daily life, but they show in a really good way what can be achieved. \n",
    "\n",
    "The NLP goal is to design and build computer systems capable of analyzing and responding to text or voice, similar to the way humans do. In this notebook, we will take a closer look at the fundamentals of NLP and how Deep Learning has contributed to better results compared to traditional approaches. \n",
    "\n",
    "As motivation, visit the website [GPT-3 Demo](https://gpt3demo.com/) and see if you can find something interesting made with [GPT-3](https://en.wikipedia.org/wiki/GPT-3). Personally, I find this one impressive: [AI-Powered Code Generator](https://sourceai.dev/documentation/example#example-in-java)\n",
    "\n",
    "\n",
    "## Learning goals\n",
    "- understanding NLP and its applications\n",
    "- understanding how to represent textual data\n",
    "- deep learning in the context of NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e5ae52",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36c03964f0f31e533f791231c375e845",
     "grade": false,
     "grade_id": "cell-082c954662c82884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Recommended Reading\n",
    "<a id='hapke'></a>\n",
    "- Hapke, H., Howard, C. and Lane, H., 2019. **Natural Language Processing in Action: Understanding, analyzing, and generating text with Python**. Simon and Schuster. \n",
    "\n",
    "-  Beysolow II, Taweh. **Applied Natural Language Processing with Python. Implementing Machine Learning and Deep Learning Algorithms for Natural Language Processing**. 1st ed. 2018., Apress, 2018, doi:10.1007/978-1-4842-3733-5.\n",
    "\n",
    "\n",
    "## Additional Material (Optional)\n",
    "\n",
    "- [CS224N: Natural Language Processing with Deep Learning](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z)\n",
    "- [Natural Language Processing (NLP) Zero to Hero](https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S)\n",
    "- [Natural Language Processing - Stanford University](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv)\n",
    "- [Natural Language Processing Specialization\n",
    "](https://www.deeplearning.ai/program/natural-language-processing-specialization/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7795c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3efeab98570b3a9bd88a08fb5f330a25",
     "grade": false,
     "grade_id": "cell-85e4533b4cfc16ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " # What is Natural Language Processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842c1208",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d72d53943b437725c141397fa626ef65",
     "grade": false,
     "grade_id": "cell-7f5375990056c8d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "NLP is a field of Computer Science dealing with methods to analyze, model, and understand human language. It is composed of tasks like: \n",
    "\n",
    "* **Speech recognition (or speech-to-text)**: this is what Google uses for converting your voice into text when you are dictating a short message to a friend on your phone or when you do a voice search in the Google app. There are many factors that make this task challenging because different people have different ways of speaking (tone, pronunciation, emphasis) and also, we need to handle improper uses of language (grammatical errors) and background noise.   \n",
    "\n",
    "* **Part-of-speech tagging**: Have you ever wondered how applications like [Grammarly](https://grammarly.com/) or [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) can check the grammar in the texts we write? In order to be able to do their job, they use a process (among others) for determining the part-of-speech (PoS) tagging of a particular word or piece of text based on its use and context. If you want to see a demo, check this website https://huggingface.co/flair/pos-english. The following picture gives you an example. \n",
    "\n",
    "![PoS Tagging](../../../coursedata/R5/post-tagging.png)\n",
    "  \n",
    "\n",
    "* **Word sense disambiguation**: Words have several meanings, for example, if I say the word '*banco*' to a Spanish-speaking person, her or his first thought would most probably be a bank (financial institution), but I could be talking about a park bench, or maybe I was thinking of a shoal of fish. The point is that the meaning of a word most of the time is subject to the context in which it is used. Word sense disambiguation is the selection of the meaning of a word with multiple meanings through a process of semantic analysis. Semantic analysis is the process of drawing meaning from text. It allows computers to understand and interpret sentences, paragraphs, or whole documents, by analyzing their grammatical structure, and identifying relationships between individual words in a particular context.\n",
    "\n",
    "* **Named entity recognition**: Named entities are sets of elements that are relevant to understanding a text. Named Entity Recognition (NER) is the process of finding entities that can be put under categories like names, organizations, locations, quantities, monetary values, percentages, etc. In the example given in the figure, what do you think is more useful, having *Aalto* and *University* as two separate words or having *Aalto University* as a unit? To be fair, the answer depends on your final goal, but being able to recognize that \"Aalto University\" is an organization, or that \"Alex\" is a proper noun is extremely useful when you are creating relationships between entities.\n",
    "\n",
    "![NER](../../../coursedata/R5/ner.png)\n",
    "\n",
    "* **Sentiment analysis**: If you are a company that sells, let's say, bikes, you most probably are interested in knowing how your customers feel about the quality of your bikes. But maybe you want to go even further and you are not only interested in the opinions of the bikes as a whole, but you want to know what people think about the brakes, or the wheels or the crank arm. All this information could be extracted from the reviews that customers give you and being able to analyze it properly is possible with sentiment analysis.\n",
    "\n",
    "* **Natural language generation**:  Nowadays, chatbots are present on many websites. They try to guide us through the website, or answer frequently asked questions. Some of them are just rule-based, but you can find some with the ability to generate text. These chatbots are known as conversational systems and their final goal is to generate a text that can sound human-produced.\n",
    "\n",
    "There are many other tasks within NLP, like **Topic Modeling**, **Information Retrieval**, **Question and Answering**, **Image Captioning**, etc., but because this is an introductory course, we cannot cover all of them. If you want to get a broader introduction, consult this article [Natural Language Processing (NLP)](https://www.ibm.com/cloud/learn/natural-language-processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a2b120",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "83bac90397b15b6b54f0ea712a77de99",
     "grade": false,
     "grade_id": "cell-387fbca956fa60a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Document representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1e2f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "940bcbe6fcceb398b2b2173cb54c9cb4",
     "grade": false,
     "grade_id": "cell-c32b7f6a245562ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The document representation is a crucial step in any task related to NLP. Traditionally, the model used to represent documents as vectors are called **Vectorial Space Model**. This model is based on the idea that words in a document aren't related, hence, the document is just a *bag-of-words*. This is a really easy and simple model to implement, but it is good to know that it has some disadvantages:\n",
    "\n",
    "* The meaning and the structure of documents cannot be expressed.\n",
    "* Each word is independent of the others, word sequences or any other type of relationship cannot be expressed.\n",
    "* If two documents have similar meanings but different vocabularies, calculating the similarity between the two of them can be difficult.\n",
    "\n",
    "Given those cons, other models for document representation have been developed, some of which are:\n",
    "\n",
    "* **Latent Semantic Analysis (LSA)**: It is based on the idea that (1) meaning is contextually dependent and (2) in the contextual use, there are semantic relationships that are latent. Read more in:\n",
    "   > Martin, D.I. and Berry, M.W., 2007. *Mathematical foundations behind the latent semantic analysis. Handbook of latent semantic analysis*, pp.35-56.\n",
    "* **Probabilistic Latent Semantic Analysis (PLSA)**: It is a statistical technique based on the general model of latent variables and it's an alternative to the LSA. Read more in:\n",
    "   > Hofmann, T., 2013. *[Probabilistic latent semantic analysis](https://arxiv.org/ftp/arxiv/papers/1301/1301.6705.pdf)*. arXiv preprint arXiv:1301.6705.\n",
    "   \n",
    "* **Latent Dirichlet Allocation (LDA)**. It was proposed to address the shortcomings of PLSA using probabilistic modeling. Probabilistic modeling assumes that the results of observations come from a generative model in which there are variables that we cannot observe (latent variables). In the case of documents, the latent variables represent the thematic structure of the documents. Read more in:\n",
    "   > Blei, D.M., Ng, A.Y. and Jordan, M.I., 2003. *[Latent Dirichlet allocation. the Journal of Machine Learning research](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?TB_iframe=true&width=370.8&height=658.8)*, 3, pp.993-1022. \n",
    "\n",
    "* **Random Indexing**: This is an incremental model of word space that is based on the accumulation of context vectors from the occurrence of words in contexts. Read more in:\n",
    "   > Sahlgren, M., 2005. *[An introduction to random indexing](https://www.diva-portal.org/smash/get/diva2:1041127/FULLTEXT01.pdf)*. In Methods and applications of semantic indexing workshop at the 7th international conference on terminology and knowledge engineering.\n",
    "* **Language Models**: A statistical model of the language is nothing more than a probability distribution P(s) over the possible sentences, expressions, documents, or any other linguistic unit of the language. A correctly adjusted model will assign high probabilities to well-formed language sentences and low probabilities to infrequent or badly formed sentences. Read more in:\n",
    "   > Rosenfeld, R., 2000. *[Two decades of statistical language modeling: Where do we go from here?](https://kilthub.cmu.edu/articles/Two_Decades_of_Statistical_Language_Modeling_Where_Do_We_Go_From_Here_/6611138/files/12103316.pdf)*. Proceedings of the IEEE, 88(8), pp.1270-1278."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d196bc9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8763957c7c79d92fb781e49c7237cc62",
     "grade": false,
     "grade_id": "cell-2d18a855ef78a61e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Bag-of-Words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddafca6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2928143c9c11a7af19ef40de0b7b123",
     "grade": false,
     "grade_id": "cell-e1821c51ada527c8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "This model assumes that the document is a vector from a vocabulary $V=[w_1,w_2,\\dots,w_{|V|}]$ and that the values of the components of the vector are the frequency of the **i-th** word of the vocabulary in the document.\n",
    "\n",
    "Let's illustrate the model with an example. Let's assume that we have the following vocabulary and set of documents:\n",
    "\n",
    "```python\n",
    "V = [\n",
    "  'aalto', 'art', 'bold', 'build', 'business', \n",
    "  'challenges', 'community', 'creating', 'future', 'global',\n",
    "  'is', 'major', 'meet', 'novel', 'science', \n",
    "  'solutions', 'sustainable', 'technology', 'thinkers', 'university'\n",
    "]\n",
    "\n",
    "documents = [\n",
    "    \"Aalto University is a community of bold thinkers where science and art meet technology and business\",\n",
    "    \"We build a sustainable future by creating novel solutions to major global challenges\",\n",
    "]\n",
    "```\n",
    "$|V| = 20$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38bc8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bde30cc101df37acad4a7b429cc2f698",
     "grade": false,
     "grade_id": "cell-34c358c9cd6f8259",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Then each document will be represented by a vector of 20 components, which is the length of the vocabulary. This vector will be sparse, which means that most of its components will be zero. The vectorial representation of our documents is:\n",
    "\n",
    "```python\n",
    "\n",
    "X = [\n",
    "   [1, 1, 1, 0, 1,   0, 1, 0, 0, 1,   1, 0, 1, 0, 1,  0, 0, 1, 1, 1],\n",
    "   [0, 0, 0, 1, 0,   1, 0, 1, 1, 0,   0, 1, 0, 1, 0,  1, 1, 0, 0, 0],\n",
    "]\n",
    "```\n",
    "\n",
    "The value in `X[0][0]` represents the frequency of the word `aalto` in the first document. Another common pre-processing task is to calculate the TF-IDF, which stands for **Term Frequency - Inverse Document Frequency**. This is a technique for quantifying a word in multiple documents. We generally compute a weight to each word signifying the importance of the word in the document and corpus. The more frequent the word is in the corpus, the lower is its corresponding component in the vector. Let's say we want to classify documents, then we would like to assign more weight to words that occur only inside a document and not too frequently in the rest. Doing so, we will get vectors that characterize documents with similar meanings.\n",
    "\n",
    "Its mathematical formulation is:\n",
    "\n",
    "$\\text{idf}(t, D) = log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$\n",
    "\n",
    "$\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\times \\text{idf}(t, D)$\n",
    "\n",
    "\n",
    "where:\n",
    "\n",
    "* $\\text{tf}(t, d)$ is the frequency of the term $t$ in the document $d$. It can be defined as:\n",
    "    - boolean frequency: $\\text{tf}(t, d) = 1$ if $t$ is in $d$, zero otherwise\n",
    "    - logarithmic frequency: $\\text{tf}(t, d) = 1 + \\log{f(t, d)}$; $f(t, d)$ is the amount of times that $t$ is in $d$. If $t$ is not in $d$, then its         $\\text{tf}(t, d) = 0$\n",
    "    - normalized frequency: $\\text{tf}(t, d) = \\frac{f(t, d)}{max\\{f(x, d): x \\in d\\}}$; $f(t, d)$ is defined as in the logarithmic frequency\n",
    "   \n",
    "* $|D|$ is the number of documents in the corpus\n",
    "* $|\\{d \\in D: t \\in d\\}|$ is the number of documents containing the term $t$\n",
    "\n",
    "For a more comprehensive explanation, check these resources:\n",
    "\n",
    "* Towardsdatascience blog post [TF-IDF from scratch in python on real world dataset.](https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089)\n",
    "* Wiki [tfâ€“idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "* Kdnuggets blog post [WTF is TF-IDF?](https://www.kdnuggets.com/2018/08/wtf-tf-idf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b726534",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5003ba5b690ae41ae2a7fb51c2dac06c",
     "grade": false,
     "grade_id": "cell-455b4d68888bd9c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Python Libraries for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0904a532",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4172ddcebb5a18001182d849bc2cf12b",
     "grade": false,
     "grade_id": "cell-085f1a2485981a79",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are many high-performance libraries for NLP in the Python ecosystem. Some of the more popular ones are:\n",
    "\n",
    "* [**Natural Language Toolkit (NLTK)**](https://www.nltk.org/): It is a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning. It is an open-source library licensed under the [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0) license. \n",
    "* [**SpaCy**](https://spacy.io/): It is described as a production-ready training system with support for 64+ languages and integration with Deep Learning frameworks like  PyTorch and TensorFlow. It is an open-source library licensed under the [MIT](https://mit-license.org/) license.\n",
    "* [**Gensim**](https://radimrehurek.com/gensim/): It is oriented to Topic Modeling. It offers implementations for  Word2Vec, FastText, Latent Semantic Indexing (LSI, LSA, LsiModel), Latent Dirichlet Allocation (LDA, LdaModel), etc. It is an open-source library licensed under the OSI-approved [GNU LGPLv2.1](https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html) license (free for both personal and commercial uses). \n",
    "* [**Sklearn**](https://scikit-learn.org/stable/index.html): It is described as a simple and efficient tool for predictive data analysis. It implements several algorithms for classification, regression, clustering, dimensionality reduction, model selection, pre-processing. It is an open-source library licensed under the [BSD](https://opensource.org/licenses/BSD-3-Clause) license.\n",
    "* [**Keras**](https://keras.io/): It provides methods that allow designing and training of an ANN using a few lines of Python code. It is implemented as a wrapper for most popular deep learning frameworks like TensorFlow, Theano, and CNTK. It is an open-source library licensed under the [Apache License Version 2.0](http://www.apache.org/licenses/LICENSE-2.0) license.  \n",
    "* [**Hugging Face**](https://huggingface.co/): Hugging Face is \"the AI community building the future\". Its mission is to democratize NLP and make models accessible. It provides resources like datasets, tokenizers, and transformers to perform NLP tasks such as sentiment analysis, coreference resolution, question answering, chatbots. If you want to learn more about what you can do with this library, take a look at [Introduction to Hugging Face ecosystem](https://huggingface.co/course/chapter0?fw=tf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc6ddd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ca57c5338ab3d0e4fec8a7c67dd7ea6",
     "grade": false,
     "grade_id": "cell-b55373fc19d243d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Given that many of you used [Sklearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) in the \"Machine Learning with Python\" course, we will use it in this section to illustrate the concepts described so far. We will use [20 Newsgroups](http://qwone.com/~jason/20Newsgroups/) dataset. It is a collection of 20 different newsgroups (~20,000 documents) talking about politics, religion, science, sports, etc. One of the challenges with this corpus is that some of the groups are similar to each other in the subject matter.\n",
    "\n",
    "Let's begin by loading the corpus.  In linguistics and NLP, corpus (literally Latin for body) refers to a collection of texts. We will select four newsgroups or categories out of 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87a7c80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d85e4e5f4bd856a55cda9adc011d48ba",
     "grade": false,
     "grade_id": "cell-356a911895758bcf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_20newsgroups # import  20 Newsgroups from sklearn\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# newsgroups\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.politics.guns',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "# load train and test data from fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=rng)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc3d36",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85b60ec56481a4ef2b3cdfde6ab019d7",
     "grade": false,
     "grade_id": "cell-09799a358850087a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f'There are a total of {len(train.data)} documents in the training set')\n",
    "print(f'There are a total of {len(test.data)} documents in the test set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f55224e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08842de3044d5bbca814cfeee6ec68c1",
     "grade": false,
     "grade_id": "cell-8c953360e3d87cb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As with many other sklearn datasets, loaded sets are of bunch datatype and contain following keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b9b91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b764af132ddba7f9cde87c8d8ed2725",
     "grade": false,
     "grade_id": "cell-2294138ef353b898",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11d923",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "320b85214311434e750033fb3b40b856",
     "grade": false,
     "grade_id": "cell-21d6c446ad1c3b8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "`train.data` contains textual data, `train.filenames` contains names of the documents, `train.target_names` contains category of a file and `train.target` - numeric respresenation of that category (label). With `train.DESCR` you can print out information about dataset.\\\n",
    "Let's print out all these data for the first file in the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb8cc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f67d55143add1630202c6ff31fa03be2",
     "grade": false,
     "grade_id": "cell-4007c78b399fd46b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Data: {train.data[0][:40]}\\n\")                 # print first 40 symbols of first file in train set\n",
    "print(f\"Filename: {train.filenames[0]}\\n\")\n",
    "print(f\"Category: {train.target_names[0]}\\n\")\n",
    "print(f\"Category, numeric label: {train.target[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087322c2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9981f3b4cc85b9056e7d2584119accf4",
     "grade": false,
     "grade_id": "cell-abb3ad679802e7f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Summary of train and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98844cb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5402a8dab17e0944765ec41aa8a0569",
     "grade": false,
     "grade_id": "cell-b23c3b7d779f35ab",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter \n",
    "import pandas as pd\n",
    "\n",
    "# create a counter object from targets (category) of train and test sets\n",
    "train_counter = Counter(train.target)\n",
    "test_counter =  Counter(test.target)\n",
    "\n",
    "# create dataframe with counted n.o. files belonging to a certain category\n",
    "cl = pd.DataFrame(data={\n",
    "    'Train': { **{ train.target_names[index]: count for index, count in train_counter.items()}, 'Total': len(train.target)},\n",
    "    'Test':  { **{test.target_names[index]: count for index, count in test_counter.items()},  'Total': len(test.target)},\n",
    "})\n",
    "\n",
    "cl.columns = pd.MultiIndex.from_product([[\"Class distribution\"], cl.columns])\n",
    "cl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf28411",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d935cd26b1f501881485dd3de8cfd8c1",
     "grade": false,
     "grade_id": "cell-39633f9d13e82a4d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's have a closer look at the structure of our documents with interactive jupyter widget. You do not need to understand the code, just check its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac327a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f86c22876e04757f3c6054f41e739cc7",
     "grade": false,
     "grade_id": "cell-a679333c1ffab68a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "max_index = 15 # You can adjust this value if you want to check more documents\n",
    "dropDown = widgets.Dropdown(\n",
    "    options=[f'{i + 1}' for i in range(max_index)],\n",
    "    value='1',\n",
    "    disabled=False,\n",
    "    layout={'width': 'max-content'}\n",
    ")\n",
    "\n",
    "classLabel = widgets.Label(value=train.target_names[train.target[int(dropDown.value)]])\n",
    "\n",
    "documentContent = widgets.HTML(value=f'<textarea rows=\"25\" cols=\"120\" readonly>{train.data[int(dropDown.value)]}</textarea>')\n",
    "\n",
    "def handle_dropdown_change(change):\n",
    "    index = int(change.new) - 1\n",
    "    text = train.data[index]\n",
    "    dclass = train.target_names[train.target[index]]\n",
    "    \n",
    "    classLabel.value = dclass\n",
    "    documentContent.value = f'<textarea rows=\"25\" cols=\"120\" readonly>{train.data[int(dropDown.value)]}</textarea>'\n",
    "\n",
    "dropDown.observe(handle_dropdown_change, names='value')\n",
    "\n",
    "items = [widgets.Label(value='Document index:'), dropDown, widgets.Label(value='Document class:'), classLabel, documentContent]\n",
    "widgets.GridBox(items, layout=widgets.Layout(grid_template_columns=\"repeat(2, 130px)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d7d9e8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00caf84687194523a6014c526342a8c7",
     "grade": false,
     "grade_id": "cell-59a27a3802c2100f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As you can see, the documents contain a lot of characters that are not useful for our analysis. Characters like `-`, `>` or `|` do not have any semantic weight and they can be safely removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb961d33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5c00bb50c1f2086a6a925df6ae90af0f",
     "grade": false,
     "grade_id": "cell-0f5deb769908ec97",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St1'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    <h3><b>DEMO.</b> DOCUMENT REPRESENTATION. </h3>\n",
    "    \n",
    "Here we will demonstrate how to transform text to a TF-IDF-weighted document-term matrix.  \n",
    "First, we need to convert a collection of text documents to a matrix of token counts. We do it with the sklearn `CountVectorizer` class.\\\n",
    "Then we create a weighted version of this vector with the `TfidfTransformer` class.\\\n",
    "We chain two steps with the sklearn `Pipeline` class.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4ba191",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2af3b3b0d66d28de43c2ba175969639",
     "grade": false,
     "grade_id": "cell-fa5b1df7e6f9fd99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# text document\n",
    "corpus = ['this is the first document',\n",
    "          'this document is the second document',\n",
    "          'and this is the third one',\n",
    "          'is this the first document']\n",
    "# vocabulary\n",
    "vocabulary = ['this', 'document', 'first', 'is', 'second', 'the',\n",
    "              'and', 'one']\n",
    "\n",
    "# create tokens from text given vocabulary (i.e. create the count vectorizer)\n",
    "token_matrix = CountVectorizer(vocabulary=vocabulary)\n",
    "# convert count matrix to TF-IDF format (i.e.create the tfi-df trasformer)\n",
    "tfid_transform = TfidfTransformer()\n",
    "\n",
    "# chain steps\n",
    "pipe = Pipeline([('count', token_matrix),\n",
    "                 ('tfid', tfid_transform)])\n",
    "\n",
    "# fit data\n",
    "pipe.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ed19ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fe9a7cc48cbb9165b9d9e555062a589",
     "grade": false,
     "grade_id": "cell-4ef2917b100b5f71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Display tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5615199",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fd1b4f61561efaf16d92710e65de5d7",
     "grade": false,
     "grade_id": "cell-2e50a77a8719ad01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipe['count'].transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80b1dd7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39f4de658c7ab124ddae105b755f49b9",
     "grade": false,
     "grade_id": "cell-4f4e34168506e7bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Display text converted to TF-IDF representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51f09a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7a56dc97b9db01fae11618fe8abc91e4",
     "grade": false,
     "grade_id": "cell-1faca0bab1d14bd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipe.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1330cb83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfdfe7a23e4592439605fcf623f883e8",
     "grade": false,
     "grade_id": "cell-dba99c10eb1c84c4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 6.1.</b> DOCUMENT REPRESENTATION. </h3>\n",
    "    \n",
    "Your task is to implement a pre-processing pipeline to convert documents to vectors using bag-of-words and TF-IDF.    \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b6a1b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea63f9424a21149aa1d0b7e0fead12c9",
     "grade": false,
     "grade_id": "cell-ea8acddcda460237",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "To implement the `text_processing_pipeline()` function you will need to perform the following steps:\n",
    "\n",
    "* Create a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) which tokenizes the documents and creates vectors of word counts. Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens. These tokens are often loosely referred to as terms or words, but they could be words, numbers, acronyms, word-roots, or fixed-length character strings.\\\n",
    "You should set the parameter `stop_words` to `\"english\"` and take into consideration the parameter `features` of the `text_processing_pipeline()` function.\\\n",
    "We provide you a custom function `preprocess_text`, which removes unimportant symbols, to be set as the argument `preprocessor` of the `CountVectorizer` object. \n",
    "* Create a [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to normalize the word counts matrix using TF-IDF. The default parameters are sufficient in this case.\n",
    "* Create a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to perform both operations in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9d467",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f067ebd418553f59e1ebfcedd95d16bc",
     "grade": false,
     "grade_id": "cell-a253063b729286fa",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)        # Remove numbers\n",
    "    text = re.sub(r'[-_]+', '', text)      # Remove lines like: \"-----------------\" or \"______________\"\n",
    "    text = re.sub(r'\\/*\\|+|\\/+', '', text) # Removes combinations like \"/|\", \"||/\" or \"////\"\n",
    "    return text\n",
    "\n",
    "def text_processing_pipeline(features=None):\n",
    "    '''  Corpus pre-processing pipeline\n",
    "    \n",
    "    The inputs to the function are:\n",
    "      - list of strings (one element is a document)\n",
    "      - maximum number of features (size of the vocabulary) to use\n",
    "      \n",
    "    It returns a Pipeline object   \n",
    "    '''\n",
    "     # YOUR CODE HERE\n",
    "     raise NotImplementedError()\n",
    "    \n",
    "    # create the count vectorizer\n",
    "    # vectorizer = ...\n",
    "    \n",
    "    # create the tfi-df trasformer\n",
    "    # tfidf = ...\n",
    "    \n",
    "    # create the pipeline\n",
    "    # pipeline = ...\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a860a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9d7316f6b78022d389488cc3b70d097",
     "grade": false,
     "grade_id": "cell-29b0254e5d5d38e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can use text processing pipeline to create training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911983a2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "44ef74515049fb9a5418a58550a86d9f",
     "grade": false,
     "grade_id": "cell-7ee7bc1993525ee4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipeline = text_processing_pipeline(features=10000)\n",
    "X_train = pipeline.fit_transform(train.data)\n",
    "y_train = train.target\n",
    "\n",
    "X_test = pipeline.transform(test.data)\n",
    "y_test = test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a9e4c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ea315c5f1d5d3facad8c851f3610922",
     "grade": false,
     "grade_id": "cell-f02b6fafb7612bfe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "assert X_train.shape==(2203, 10000)\n",
    "assert X_test.shape==(1466, 10000)\n",
    "\n",
    "print(\"Sanity check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9dafc2-bec1-4c8f-a710-0afa3aeaab7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7bac8ec8526c6bf54b9a3be451c4843",
     "grade": true,
     "grade_id": "cell-a2d5a3fbce5afd9d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37bf83-3da5-4274-85db-30bba6a049ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e72971c9bb2f731330f1202e49c9c16",
     "grade": true,
     "grade_id": "cell-2383edd74af24a0e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4aed88-40bf-4ca1-aa73-afdef153e9f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6515bd95e803cc536485b38ef843dbd8",
     "grade": true,
     "grade_id": "cell-6070ac98f3dad809",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458e5d9-61e2-4ded-9a33-55779c8305bc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15787525b3e0671ea88489e653438582",
     "grade": true,
     "grade_id": "cell-5a8a85abd4968d9b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7051ceb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "114cf24c6a317b6b6d6d7f4e3b41a79a",
     "grade": false,
     "grade_id": "cell-84ca2b2ab829d32d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='St2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>STUDENT TASK 6.2.</b> DOCUMENT CLASSIFICATION. </h3>\n",
    "    \n",
    "Your task is to train a [LogisticRegression classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and evaluate its performance over the test data. Expected f1_score on test set is ~0.94.\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9969835",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "127a641d273bcc24aeae0b1d89312227",
     "grade": false,
     "grade_id": "cell-e89e9a6b59648c3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hints:**\n",
    "\n",
    "Because we are doing multi-class classification, you need to pass the parameter `average` to the `f1_score`. You can choose between `{'micro', 'macro', 'samples', 'weighted'}` but we suggest `'weighted'`. Read more about it in the documentation for the [f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) function.\n",
    "\n",
    "Important: Store the predictions in a variable called `pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c66fa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b3f338fb7303ebece2c628e4b47a40",
     "grade": false,
     "grade_id": "cell-bcf747eb26df5e1e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# define classifier with sklearn LogisticRegression\n",
    "# clf = ...\n",
    "\n",
    "# fit classifier to training set\n",
    "# clf...\n",
    "\n",
    "# get predictions for test set\n",
    "# pred = ...\n",
    "\n",
    "score = accuracy_score(y_test, pred)\n",
    "print(\"Accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1 = f1_score(y_test, pred, average='weighted')\n",
    "print(\"      F1:   %0.3f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f33903addd65ded64a1e67edeed2e6ce",
     "grade": true,
     "grade_id": "cell-d3ace11bcf8f9962",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06506d81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ef4e00ec24d56466fb1bceefb1c34aff",
     "grade": false,
     "grade_id": "cell-c23d983959ef7a1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you implemented everything correctly you should get precision and F1 around **0.94**. This is a really good result, but keep in mind that we are only using 4 classes that are properly separated. Let's now visualize the confusion matrix. If you have problems understanding what it shows, take a look at this article [Understanding Confusion Matrix](https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62). One thing to notice is that Sklearn flips the values, and we get the matrix in the following form:\n",
    "\n",
    "\n",
    "<img src='../../../coursedata/R5/confussion_matrix.png' width=400/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f18786",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b21628138a5bc2dfc1fcd1d1b56e899f",
     "grade": false,
     "grade_id": "cell-987b1b3750dfca0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Code source https://stackoverflow.com/questions/62722416/plot-confusion-matrix-for-multilabel-classifcation-python\n",
    "def print_confusion_matrix(confusion_matrix, axes, class_label, class_names, fontsize=14):\n",
    "\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, ax=axes)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "        \n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    axes.set_ylabel('True label')\n",
    "    axes.set_xlabel('Predicted label')\n",
    "    axes.set_title(\"Confusion Matrix for the class - \" + class_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5233fb57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6f939aa932a1e4379c02ab5f3cd59d77",
     "grade": false,
     "grade_id": "cell-1f69894d6bca8643",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "cfs_matrix = multilabel_confusion_matrix(y_test, pred)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 7))\n",
    "    \n",
    "for axes, cfs, label in zip(ax.flatten(), cfs_matrix, train.target_names):\n",
    "    print_confusion_matrix(cfs, axes, label, [\"N\", \"P\"])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135e2e0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "09b4329d2cf2311a1ea915911becf025",
     "grade": false,
     "grade_id": "cell-14aaa3ee0e615ac2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a6843",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "65f80df9bc80a4d54a5d3cd3111a4497",
     "grade": false,
     "grade_id": "cell-6b7590dbde1b2cb4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The techniques discussed above only consider the linear relationships between words, and in many cases, you need an expert to define the features to use in each task. With Artificial Neural Networks (ANN) we can accomplish feature extraction in a completely automated way. We saw that the Bag-of-Word model ignores the context in which a word is used. This means that it also ignores the effect that the neighbors of a word have on its meaning and in the whole meaning of a statement.\n",
    "\n",
    "When we use Bag-of-Word, each word  is represented with a vector that is the result of a one-hot-encoding. For example, let's say that our vocabulary is composed of the following words:\n",
    "\n",
    "```python\n",
    "V = ['dog', 'painting', 'sun', 'winter', ]\n",
    "```\n",
    "then the word vectors for each words are:\n",
    "\n",
    "```python\n",
    "word_vector['dog']      = [1, 0, 0, 0]\n",
    "word_vector['painting'] = [0, 1, 0, 0]\n",
    "word_vector['sun']      = [0, 0, 1, 0]\n",
    "word_vector['winter']   = [0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "It is easy to see that these vectors are orthogonal, which means that there is not a natural notion of similarity between them. Using ANN, it is possible to build dense vectors for each word that can capture the concept of synonyms, antonyms, or words that just belong to the same category, such as people, animals, places, etc. These vectors are called word vectors or word embeddings and [Hapke](#hapke) define them as:\n",
    "\n",
    "\n",
    "> Word vectors are numerical vector representations of word semantics or meaning, including literal and implied meaning. So word vectors can capture the connotation of words, like \"peopleness\", \"animalness\", \"placeness\", \"thingness\", and even \"conceptness\". And they combine all that\n",
    "into a dense vector (no zeros) of floating-point values. This dense vector\n",
    "enables queries and logical reasoning.\n",
    "\n",
    "The following picture shows a 3D projection of the embeddings calculated using Google's [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec#embedding_lookup_and_analysis). To create the graph, they first calculate [PCA](https://machinelearningmastery.com/principal-components-analysis-for-dimensionality-reduction-in-python/) and then position the words according to the cosine distance between the vectors in the original space.\n",
    "\n",
    "![Word Embedings visualization](../../../coursedata/R5/w2v-visualization.png)\n",
    "\n",
    "The highlighted word is **machine**. As we expect, words like *learning*, *computer* and *translation* are among the 100 more similar words. In this case, the embedding has captured the use of machine in the Machine Learning field. But we can also see other similar words like *gun*, *weapon*, *rifle* and *pistol* which belong to a completely different domain. In the website [Embedding Projector](https://projector.tensorflow.org/) you can explore more and play with different representations.\n",
    "\n",
    "But, how exactly these vectors are calculated? Word2Vec was developed by [Tomas Mikolov in 2013 at Google](https://arxiv.org/pdf/1310.4546.pdf). The goal is to create a model that can learn high-quality word vectors from **huge** data sets, typically billions of words, and millions of (unique) words in the vocabulary. This is achieved with the basic task of being able to predict what words occur in the context of other words. \n",
    "\n",
    "![Word2Vec Intuition](../../../coursedata/R5/w2v_intuition.png)\n",
    "\n",
    "In the figure, the words in the green rectangles are in the context, and the center word is *community*. The analysis happens in a *sliding window* which in this case is of size 2. Two models can be used: Continuous Bag-of-Words (CBOW) and Continuous Skip-gram.\n",
    "\n",
    "![Word2Vec Architectures](../../../coursedata/R5/word2vec_architectures.png)\n",
    "\n",
    "1. Continuous Bag-of-Words: Given its context, the goal is to predict the center word. It is faster than Skip-gram and has better representations for more frequent words.\n",
    "2. Continuous Skip-gram: Given the center word, the goal is to predict the context. This method works well with a small amount of data and is found to represent rare words well.\n",
    "\n",
    "The ANN architecture for both models is really simple, and reassemble what was shown in the previous picture: the input layer, a hidden layer, and the output layer.\n",
    "\n",
    "![Word2Vec ANN](../../../coursedata/R5/w2vec_ann.png)\n",
    "\n",
    "Because of time constraints, we won't offer a full derivation, but professor [Christopher Manning](https://nlp.stanford.edu/~manning/) gives an extensive explanation of the process in the video [Introduction and Word Vectors](https://youtu.be/rmVRLeJRkl4) which is the first lecture for the [Stanford CS224N NLP with Deep Learning ](https://web.stanford.edu/class/cs224n/index.html) course. \n",
    "\n",
    "We will be using pre-trained English word embeddings, but the [Turku NLP Group](https://turkunlp.org/) have trained a model for the Finnish language that you can download in this link: \n",
    "\n",
    "http://dl.turkunlp.org/finnish-embeddings/. \n",
    "\n",
    "You can find more info on their website [Finnish NLP](https://turkunlp.org/finnish_nlp.html). In the website [NLPL word embeddings repository](http://vectors.nlpl.eu/repository/), maintained by the  University of Oslo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf581a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8445c3a28f53ebfd1b29238884c1fbe7",
     "grade": false,
     "grade_id": "cell-18e54852b67da5d4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Beside word similarity, word embeddings also allows us to calculate *word analogies*. The classical example of this is:  $\\vec{\\text{king}} - \\vec{\\text{man}} + \\vec{\\text{woman}} = \\vec{\\text{queen}}$. \n",
    "\n",
    "In the article [Word Embedding Analogies: Understanding King - Man + Woman = Queen](https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html), the author explains why these calculations hold. We recommend you to read the paper [Towards Understanding Linear Word Analogies](https://arxiv.org/pdf/1810.04882.pdf). The Turku NLP Group have an online demo where you can play with word analogies on the website:\n",
    "\n",
    "http://bionlp-www.utu.fi/wv_demo/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39303bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da4f3f87fb9c14ac6a6135f0dadbef87",
     "grade": false,
     "grade_id": "cell-e8ca79bfa8b3c6f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Document classification with word embeddings\n",
    "\n",
    "Let's see how we can use word embedding to classify documents. The [complete pre-trained model](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) is a binary file of ~3.4 Gb. This means that you will need at least 16 Gb of RAM on your computer to load the whole file. We have extracted a subset with 20,000 words present in the collection we are working with. If you are interested in training your own model, take a look at the tutorial [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) from the Tensorflow documentation.\n",
    "\n",
    "The first step is to load the subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a7e5e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d16f9a99b418a6c226bb267fc2d5252d",
     "grade": false,
     "grade_id": "cell-b9a751a8de7b6b01",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "embeddings_path = Path().cwd() / '..' / '..' / '..' / 'coursedata' / 'R5' / '20newsgroups_subset_vocabulary_embeddings.p'\n",
    "\n",
    "with open(embeddings_path, \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "    vocabulary = list(embeddings.keys())\n",
    "    \n",
    "print(f'The vocabulary has a total of {len(vocabulary)} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78284e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8396ad601b25464c110cd927ac1788e2",
     "grade": false,
     "grade_id": "cell-00ae9282e8aaa21c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The variable `embeddings` is a dictionary consisting of (key, value) pairs, where the key is a word and value is the embedding vector of a word. The embedding vector is a numpy array of length 300. For example, for index 7 we have (key, value) or (word, embedding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6f197",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "860a3bcaf4c33ce72f2c77cc4cac0fa6",
     "grade": false,
     "grade_id": "cell-248e48b460d4f9d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Word: {list(embeddings.keys())[7]}\")  \n",
    "print(f\"\\nEmbedding vector shape: {list(embeddings.values())[7].shape}\") \n",
    "print(f\"\\nEmbedding vector (first 50 values): \\n\\n {list(embeddings.values())[7][:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be9bc3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a1d47c720866f5924ed39a97f8313da",
     "grade": false,
     "grade_id": "cell-3b2316a0e35fcb77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We create training and validation subsets from our `train` dataset (loaded from sklearn `fetch_20newsgroups` in the beginning of the notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b34419",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "993045507f3b4184bd873e838f0854bc",
     "grade": false,
     "grade_id": "cell-42213a59005db6a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Extract a training & validation split\n",
    "validation_split = 0.2\n",
    "num_validation_samples = int(validation_split * len(train.data))\n",
    "\n",
    "train_samples = train.data[:-num_validation_samples]\n",
    "val_samples = train.data[-num_validation_samples:]\n",
    "\n",
    "train_labels = train.target[:-num_validation_samples]\n",
    "val_labels = train.target[-num_validation_samples:]\n",
    "\n",
    "test_samples = test.data\n",
    "test_labels = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a75c772",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b590e468006bd7b9b41e581602f97965",
     "grade": false,
     "grade_id": "cell-0e05d55d3c24e2d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "  To process the raw text, we will use the Keras layer [TextVectorization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization). This is a simple way of performing the pre-processing step, but bear in mind that you cannot use it if you need more complex pre-processing, for example, [lemmatization](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/), [stemming](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python), etc. Because we already have a vocabulary, we can pass it to TextVectorization and make sure that only words coming from it are part of our documents. In practice, you may not have a pre-defined vocabulary. In such a case, you will need to \"learn\" vocabulary with `.adapt()` method:\n",
    "\n",
    "```python\n",
    "vectorizer = TextVectorization(\n",
    "    output_mode=\"int\",\n",
    "    max_tokens=20000,\n",
    "    output_sequence_length=1000\n",
    ")\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(32)\n",
    "vectorizer.adapt(text_ds)\n",
    "```\n",
    " \n",
    "  The function `TextVectorization.adapt` is similar to the the function `fit` in the Sklearn models. According to the documentation: \"Fits the state of the preprocessing layer to the data being passed\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2ea27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e177427a4b6bc75ba0147bea72e6546",
     "grade": false,
     "grade_id": "cell-f416b61c814d5e7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(\n",
    "    output_mode=\"int\", \n",
    "    standardize=\"lower_and_strip_punctuation\", # lowercase all the words and remove punctuation\n",
    "    output_sequence_length=500 # each document will be reduced to a vector of 500 integers\n",
    ")\n",
    "vectorizer.set_vocabulary(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b9c5f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4861180ff489c91818d822cb037855c6",
     "grade": false,
     "grade_id": "cell-bd3c3c26a5f5f0f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's vectorize a test sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54255b27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58281df790638e39d115f57206b16eb0",
     "grade": false,
     "grade_id": "cell-14fbb79ee881ee7e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"Robert Plant wrote a hell of a song\"\n",
    "\n",
    "output = vectorizer(np.array([sentence]))\n",
    "output.numpy()[0, :8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9396f365",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c68a1c3b2dee7d898363e8b07aedfc62",
     "grade": false,
     "grade_id": "cell-cba0e42ee756a654",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The index 1 corresponds to the token **'[UNK]'** which is used to represent unknown words (missing words in the vocabulary). Do you have an idea why \"a\" and \"of\" are not in the vocabulary? \n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "    <p>Both \"a\" and \"of\" are in the set of stopwords</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d182dc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4a8eb3f4b960d1c92d3e5eedd5a26686",
     "grade": false,
     "grade_id": "cell-37b9fa5666a4240b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's prepare a corresponding embedding matrix that we can use in a Keras `Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained vector for the word of index `i` in our `vectorizer`'s vocabulary. \n",
    "\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3151c71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "378578fc483e8da93ccb105bf47fad45",
     "grade": false,
     "grade_id": "cell-ba78a96b0798c7a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary() # It will include tokens like '[UNK]' and the padding token \"\"\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edbc7e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "345b6cc8806887b9c6b3a5dee1da912f",
     "grade": false,
     "grade_id": "cell-4274177ff292ba18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = len(voc)\n",
    "embedding_dim = 300 \n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(f\"Converted {hits} words ({misses} misses)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b559c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02bef3adc48f1dfafee120d78dcb6f2b",
     "grade": false,
     "grade_id": "cell-573672acbd57e5dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "  Next, we load the pre-trained word embeddings matrix into an `Embedding` layer. Note that we set `trainable=False` to keep the embeddings fixed, we don't want to update them during training. We just need the ANN to learn the weights for the other layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8889dd8f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8bdeb7067de0a01c41040187e5da848d",
     "grade": false,
     "grade_id": "cell-26356bb16f7772a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae6efb1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55658d0a50aff23495ae877bec1f1e78",
     "grade": false,
     "grade_id": "cell-d4c04abadcf2bee5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "Before we can start training the model, we need to vectorize the training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c7d0f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3368c144ae1da7cdf4ce49e5c84449e2",
     "grade": false,
     "grade_id": "cell-6a91ec8818ba6c10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val   = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "x_test  = vectorizer(np.array([[s] for s in test_samples])).numpy()\n",
    "\n",
    "y_train = train_labels\n",
    "y_val   = val_labels\n",
    "y_test  = test_labels\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Validation set shape: {x_val.shape}\")\n",
    "print(f\"Test set shape: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b765bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39e5dc04a7f80d380943bed4fd38a3cb",
     "grade": false,
     "grade_id": "cell-e2e6ae01d2ddb22a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see the result of text vectorization. As an example we select training sample with index 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c6e809",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1fc32ed509c8ddbeda1e21f66597968",
     "grade": false,
     "grade_id": "cell-2a3d7bc126b555ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Original text:\\n {train_samples[3][:103]}\\n\")\n",
    "print(f\"Vectorized text:\\n {x_train[3][:13]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e588610",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a60a225d9b0bb708f512465c4780709b",
     "grade": false,
     "grade_id": "cell-b77896d9460f00bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we check values of vectorized text in our vocabulary `voc`, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab6c3be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f94b7139a2e1b9943bd03cae43b37d34",
     "grade": false,
     "grade_id": "cell-69da82ddbfd0f8d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "voc[1], voc[302], voc[6], voc[303], voc[304],voc[305],voc[306],voc[307],voc[308]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bba78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4855c3ca6b3d482b6e94b29dbc799188",
     "grade": false,
     "grade_id": "cell-982d687fc18a660b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Thus, to each word in original text we assign a numeric value according to our vocabulary.\n",
    "\n",
    "<img src=\"../../../coursedata/R5/vectorized.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d7a0b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc968c54aaf19bc82dc983b333860e84",
     "grade": false,
     "grade_id": "cell-3cc0c0cdb6c051a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The vectorized text is then used to build the corresponding matrix with pre-trained embeddings. \n",
    "\n",
    "![Embeddings](../../../coursedata/R5/embeddings.png)\n",
    "\n",
    "We can confirm this by checking the outputs of the embedding layer we created before. For example, we have 1763 vectorized documents in the training set. Each document is of length 500 tokens. The embedding matrix consists of ~20k embedding vectors of length 300. After passing vectorized training set, we get embedding vectors for each of the tokens in the training set, 1763x500x300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34017e50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bd18dd09087b295881fca67f4467f5a",
     "grade": false,
     "grade_id": "cell-a275b771c7c58a92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# pass data to emb layer (matrix)\n",
    "x_train_emb = embedding_layer(x_train)\n",
    "\n",
    "print(f\"Training set shape: {x_train.shape}\")\n",
    "print(f\"Training set shape after embedding layer: {x_train_emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7d9439",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eead2b58e000f06b94dfaed8f18409a4",
     "grade": false,
     "grade_id": "cell-a6057e8212c9dee9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also confirm that returned data set is just corresponding embedding vectors from `embedding_matrix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92523d06",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "460302995ffbf22fc21d2d150f9aa19f",
     "grade": false,
     "grade_id": "cell-d3b03b0a9029100d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Vectorized document ind=0, token ind=2: {x_train[0][2]}\\n\")\n",
    "print(f\"Corresponding embedding vector: {embedding_matrix[5][:5]}\\n\")\n",
    "print(f\"Embedding for document ind=0, token ind=2: {x_train_emb[0][2][0:5]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89700f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a06b18d9853897ce7b5babfab7ad1b18",
     "grade": false,
     "grade_id": "cell-252b7c7a03b407d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<a id='St3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task 6.3.</b> Define a simple CNN to perform document classification.</h3>\n",
    "\n",
    "The structure of the ANN should be the following:\n",
    "* `embedding_layer` as the input layer\n",
    "* Two blocks of one [Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) (use 128 filters, a kernel of size 5 and activation function \"relu\") followed by a [MaxPool1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D) (with pool size 2)\n",
    "* A Conv1D (with the same parameters as above) followed by a [GlobalMaxPool1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) (with the default parameters)\n",
    "* A [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) with 128 units followed by a [Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout) (with 0.5 rate)\n",
    "* A Dense layer (output layer) with `m` units and activation `\"softmax\"`  \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b0bde7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1b791c62bf53df7c534158ca2dbf0ac",
     "grade": false,
     "grade_id": "cell-66eb5f735231454d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "# number of categories for classification\n",
    "m = len(categories)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "# model = ...\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc2e4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a9985346f9b060c552365a73a9e49f5",
     "grade": false,
     "grade_id": "cell-6b1a7d0ceb8df95e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the solution\n",
    "assert len(model.layers) == 10, \"There should be 10 layers!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a5e95-bf5f-4bc2-b99f-a6748c632aa3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "524e5686a3a8ce0d9f763b8063ad96df",
     "grade": true,
     "grade_id": "cell-004224ce7433095a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa97848",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc05d12a0ccdca0debb81778dd5f93ba",
     "grade": false,
     "grade_id": "cell-49e34f0405ea181d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task. </b>Compile and train model.</h3>\n",
    "   \n",
    "Your task is to:\n",
    "    \n",
    "1. Compile a model. Use categorical cross-entropy as our loss since we're doing softmax classification.\n",
    "Specifically, use `sparse_categorical_crossentropy` since our labels are integers. Use `sparse_categorical_accuracy` as metrics and optimizer 'RMSprop'.\n",
    "    \n",
    "2. Train model for 20 epochs with batch size 32. Save model as 'model.h5'.\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b153fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set trainig=False when validating or submitting notebook\n",
    "# and set training=True, when training network\n",
    "training=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9499bd47ba648c38f91154b07fa872f",
     "grade": true,
     "grade_id": "cell-b535246dbff99f5c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this hidden cell is for setting flag training=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67263e56",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56fbba11fee7009521f0992f417dd5d7",
     "grade": false,
     "grade_id": "cell-0940653d867f06de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile the model \n",
    "# model.compile(...)\n",
    "\n",
    "# model training\n",
    "if training:\n",
    "    # history = model.fit(...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "else: \n",
    "    model = tf.keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7abfb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "503e56e1cafd5858d9ee41ef3748aaa6",
     "grade": false,
     "grade_id": "cell-16eccaf84f7bb423",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "model = tf.keras.models.load_model(\"model.h5\")\n",
    "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test accuracy {:.2f}\".format(test_acc))\n",
    "assert test_acc>=0.91, \"Accuracy is too low!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9feec-2353-4f71-8760-5dbdcf3cdae9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1499e46065ad1a829d309e931ec8e15",
     "grade": true,
     "grade_id": "cell-89a980d21873ebc8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937de614",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42258a2af0c29f094824e5393b29c7d5",
     "grade": false,
     "grade_id": "cell-25d8a25833da0b80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Define an end-to-end model\n",
    "\n",
    "Now, we may want to define a `Model` object which takes a string of arbitrary length as input, rather than a sequence of indices. It would make the model much more portable\n",
    "since you wouldn't have to worry about the input preprocessing pipeline.\n",
    "\n",
    "Our `vectorizer` is a Keras layer, so it's simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ce9546",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2861bbebe6deceeba973bd19891dedfd",
     "grade": false,
     "grade_id": "cell-dff0730820110828",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# keras layer that takes a string as an input\n",
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "# vectorize string input\n",
    "x = vectorizer(string_input)\n",
    "# pass to main model\n",
    "preds = model(x)\n",
    "\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "end_to_end_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c8a91",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abce8b110a6fbaf568af12d03a1fc63c",
     "grade": false,
     "grade_id": "cell-81ebbcde2abb2d2c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using this final model, we can test the accuracy of the ANN. Note, that now we can pass text data `test.data` directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66568df4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "441b1149ac363430dadb234eb3bfa436",
     "grade": false,
     "grade_id": "cell-0c7344a6d1a0ed54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [np.argmax(prob) for prob in end_to_end_model.predict(test.data)]\n",
    "\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:   %0.3f\" % score)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"      F1:   %0.3f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafba33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d43ec8a49329e5db5258f06355466ce9",
     "grade": false,
     "grade_id": "cell-d4c9f8a8cf3fc884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "cfs_matrix = multilabel_confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 7))\n",
    "    \n",
    "for axes, cfs, label in zip(ax.flatten(), cfs_matrix, train.target_names):\n",
    "    print_confusion_matrix(cfs, axes, label, [\"N\", \"P\"])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ada33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1d76baf2da00e582490b84728c44289",
     "grade": false,
     "grade_id": "cell-a5c8e4d2cfbee3fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You should be getting an F1 score slightly below the one we get using the traditional approach. Given what you have learned in this course, try to explain what is happening here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1723a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e181a87275d633bee28898bc3b1d0b1",
     "grade": false,
     "grade_id": "cell-265ebf3deab85126",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The NLP field is vast and it's not possible to cover everything in one notebook. There is an exciting new area of study using [Transformers](https://huggingface.co/transformers/), where models like [BERT](https://arxiv.org/abs/1810.04805) and [GPT](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf) are giving mind-blowing results in many NLP tasks. We have covered the basics: pre-processing, document representation, word embeddings, and the different libraries available for creating NLP applications. Deep learning makes feature extraction easy, but the downside is that it needs a lot of text in order to train the algorithm. Thanks to Transfer Learning, we can use a general-purpose data set to perform the feature extraction, and then use the learned representations in domain-specific tasks. Libraries like [Hugging Face](https://huggingface.co/models) make it extremely easy to test state-of-the-art models, but you should not treat them as black boxes. \n",
    "\n",
    "Finally, if you want to learn more about the field, a good starting point is this specialization in Coursera: [DeepLearning.AI Natural Language Processing](https://www.coursera.org/specializations/natural-language-processing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cceb0c-92e7-4e9c-885a-8e557e519a1d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "00ab5569e43a9c38af0e0367a43fa36e",
     "grade": false,
     "grade_id": "cell-c6a7cb45438214cc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 1.</h3>\n",
    "    \n",
    "Choose the correct statement. The Bag-of-words model assumes that:\n",
    "\n",
    "1. Meaning of a word depends on the context\n",
    "2. All words are independent units\n",
    "3. Order of the words matters\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca57f7f-5d8e-4662-b344-662f012c8c8e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6588dfed1c6c755b04df6fe6fe5f2281",
     "grade": false,
     "grade_id": "cell-69dce2bd648690ff",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c372aaf-3f05-44df-ad97-ae224d5c262c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd3793f8c3aafae722126c8e989840b9",
     "grade": true,
     "grade_id": "cell-b1081ef290074c13",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_1 in [1, 2, 3], '\"answer\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad09e51d-3367-489e-989e-c618598f92a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d99aa727d4203536cd8899c9ca875c7",
     "grade": false,
     "grade_id": "cell-a4daaa8ebe807462",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 2.</h3>\n",
    "    \n",
    "Choose the correct statement. TF-IDF:\n",
    "\n",
    "1. Evaluate how often a word appears in a document\n",
    "2. It's a numeric representation of a text document\n",
    "3. Reflect how important a word is to a document in a set of documents\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1016e33-d10b-49bb-a42c-a77b581dfc30",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4951af5fd35899c7e7337fd1ca2aadb",
     "grade": false,
     "grade_id": "cell-def18386619efba1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440ac38-7c18-4ab0-a1fe-01ee34515c56",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebefab9416212776aead2b52f4c89eba",
     "grade": true,
     "grade_id": "cell-017fb495b1db0f8c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_2 in [1, 2, 3], '\"answer\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37fa024-c2a0-4711-9b6e-33eac916bf74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1aec269fe5e48d1f13516a3ce5c79130",
     "grade": false,
     "grade_id": "cell-37578fbea43c0839",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 3.</h3>\n",
    "    \n",
    "Choose the correct statement. In the notebook, we vectorized the text data which means that:\n",
    "\n",
    "1. We split the text document into words and mapped each word to an integer value\n",
    "2. Each letter in the text was converted to a float number\n",
    "3. We split the text document into words and mapped each word to a float number\n",
    "4. We mapped a whole text document to an integer value\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cbe79d-be1a-4b26-92e5-610d5f91ba13",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff3a84b103d96ddb2ba518a5d4188f55",
     "grade": false,
     "grade_id": "cell-04621214a9dcea43",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325dd5d-7a81-4853-9bbc-bddca67ab11c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ccbf290b3c28f444cb2cf6ddea68f2e",
     "grade": true,
     "grade_id": "cell-f114ee5c37abc137",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_3 in [1, 2, 3, 4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacd1c4-8a7b-4029-b5f9-dba1b8ef5bfc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47160c718f12ec4c49d645651344f83f",
     "grade": false,
     "grade_id": "cell-7779a09191ea214e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 4.</h3>\n",
    "    \n",
    "Choose the correct statement. The embedding matrix consists of:\n",
    "\n",
    "1. Text data\n",
    "2. One-hot encoded vectors corresponding to each word in a vocabulary\n",
    "3. Labels (categories) of the text documents\n",
    "4. Learnt real-valued representations (vectors) of each word in a vocabulary\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40619dc8-7997-48db-8aaa-63e2ea5ee9ba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6496b7765d768e329b44c2e9f38f2e8",
     "grade": false,
     "grade_id": "cell-6b414a744902a290",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e419f85-7488-4125-bded-987c2945323d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "728cfcd99e7b4e913deda77c5856b513",
     "grade": true,
     "grade_id": "cell-8e0cdc20553af18f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_4 in [1, 2, 3, 4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
