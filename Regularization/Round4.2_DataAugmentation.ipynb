{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066f3b43-4404-4a85-9a81-ac01e578009c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4296ce7c1c25b5388936f2acdffa02b",
     "grade": false,
     "grade_id": "cell-d265aa0b731ea343",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h1 align=\"center\">Round 4.2 - Data Augmentation</h1>\n",
    "\n",
    "\n",
    "This notebook is a part of teaching material for CS-EJ3311 - Deep Learning with Python 13.09.-17.12.2021\\\n",
    "Aalto University (Espoo, Finland)\\\n",
    "fitech.io (Finland)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1095e4-e406-4c78-89e8-55adfb0805a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c517a2092970719cf9314978a81f7fca",
     "grade": false,
     "grade_id": "cell-963a912e7f44db57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As a rule of thumb, the number of labeled data points used for the training of ANN should be at least as large as the number of tunable parameters (weights, bias terms) of ANN. Since typical ANN structures involve millions of tunable parameters, we often have too few data points in the training set. \n",
    "\n",
    "One powerful approach to train ANN on relatively small training sets is **data augmentation**. The idea behind data augmentation is to enrich or augment the original training dataset by simple variations of the training samples. Consider a deep learning method that aims at classifying an image as either \"cat\" or \"dog\". \n",
    "\n",
    "To obtain a \"cat vs. dog\" classifier we could train a deep neural network on some cat images and some dog images (the labeled training data). The idea of data augmentation is to **artificially** augment the training set by creating **synthetic** data points. For example, by rotating a training image. A rotated image of a cat is still a cat image. Moreover, a shifted (translated) cat image is also still a cat image. The same applies to dog images. A rotated or shifted dog image is still showing a dog. \n",
    "\n",
    "One method to implement data augmentation is via the concept of **data generators**. A data generator is somewhat like a programming interface that allows separating the code of the training algorithm (e.g., stochastic gradient descent) and the data storage system. This is very convenient for deep learning methods as the data can be stored in vastly different systems, ranging from a standard file system on a hard disk all the way to a cloud computing environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94645541-7fb9-43e1-99c3-be1b9d21d169",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68235dc6c7660066131130aeb38e762e",
     "grade": false,
     "grade_id": "cell-02c62f0b8db4f2c9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "- use tf.Dataset to train ANNs on large datasets. \n",
    "- understand the basic idea of data augmentation. \n",
    "- implement data augmentation using Keras preprocessing layer.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7e953-7011-48c7-bce4-70a8e78ae5fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ece5b2b13b0f07bb4a76b7eea6a3a24",
     "grade": false,
     "grade_id": "cell-63378766289a38ce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Recommended Reading\n",
    "\n",
    "-  [A survey on Image Data Augmentation for Deep Learning](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0)\n",
    "- [Data augmentation for improving deep learning in image classification problem](https://ieeexplore.ieee.org/abstract/document/8388338)\n",
    "\n",
    "\n",
    "## Additional Material (Optional!)\n",
    "\n",
    "- [Building a data pipeline](https://cs230.stanford.edu/blog/datapipeline)\n",
    "- [Data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) \n",
    "- [Image Augmentation using TensorFlow](https://benihime91.github.io/blog/deeplearning/tensorflow2.x/data_augmentation/image/2020/11/15/image-augmentation-tensorflow.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be1b2aaa-4c9a-4732-a292-81094b93a78f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a7ddffafa65fb2cd1a9179dc6793a8",
     "grade": false,
     "grade_id": "cell-4de87e3c2d8a7cb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/frank/.local/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# import Python packages and libraries\n",
    "\n",
    "import tensorflow as tf                  # provides functionality to train neural network\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np                       # provides mathematical functions to operate on arrays and matrices\n",
    "import os                                # library to interact with operating system\n",
    "import matplotlib.pyplot as plt          # library for generating plots\n",
    "import pathlib                           # library to perform filesystem interactions\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c0741-d413-487c-97fb-5987943a6805",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1236e5917243d47501fb81a2249f7995",
     "grade": false,
     "grade_id": "cell-a337fda8c86c6a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We will use \"Cats and Dogs\" dataset discussed in previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac74be7a-6c37-4307-9944-415ad96bec33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0cb4891ade77071c786e38da6da06e4",
     "grade": false,
     "grade_id": "cell-3c59bb0394ff3843",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# The path to the dataset\n",
    "base_dir = pathlib.Path.cwd() / '..' / '..' /  '..' / 'coursedata' / 'cats_and_dogs_small'\n",
    "\n",
    "# directories for training,\n",
    "# validation and test splits\n",
    "train_dir = base_dir / 'train' \n",
    "validation_dir =  base_dir / 'validation'\n",
    "test_dir = base_dir / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bb1d4-94e3-42b0-9275-f19b2e4f9eb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c061f32fdcc1928d7991e39a21268478",
     "grade": false,
     "grade_id": "cell-15946385002af04e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start by training a CNN without data augmentation. We will use similar data pipeline described in the notebook [Tensor Flow Input Pipeline](Round4.1_TensorFlowData.ipynb). First, we need to set some parameters that will help us to define the model in a generic way and experiment with different values easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03edaacc-339b-4c3c-a8fb-0b7749ab21d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05d14f824a7aa2a0bd28fc061cb838e9",
     "grade": false,
     "grade_id": "cell-29d7ce08e2fb4d25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "CLASS_NAMES = ['cats', 'dogs']\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = 150\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec2770-f503-4c00-9c16-4e45e17a1462",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a19ba27dd7612ff80dc26cd1df42cc2c",
     "grade": false,
     "grade_id": "cell-caa4d507861c05ed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For each image file, we perform following steps:\n",
    "\n",
    "* Read content of the file. This operation reads the raw bytes contained in the image as a string\n",
    "* Decode the file content. There are many image formats that we can use. For this specific example, all the images are encoded in the JPEG format. This operation will create a tensor of shape (with, height, channels)\n",
    "* Resize the image to the values defined above. This goal is to reduce the amount of processing power needed to process the dataset.\n",
    "* Scale the images. The original pixels values are between 0 and 255. In this step we normalize those values to have then in the interval [0, 1].\n",
    "* One-hot-encode the image label\n",
    "\n",
    "All this steps are performed by the function `load_image` that receives as its first argument the path to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ce7b305-cf1a-4ebd-82f5-a1f380afb523",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c78eccb114072e7f64b1f6e09eaebc86",
     "grade": false,
     "grade_id": "cell-1d749cb0caaeac9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    # load image\n",
    "    image = tf.io.read_file(image_path)    # read the image from disk\n",
    "    image = tf.io.decode_jpeg(image, channels=3)    # decode jpeg  \n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])    # resize\n",
    "    image = (image / 255)    # scale \n",
    "    \n",
    "    # get lable value from path\n",
    "    parts = tf.strings.split(image_path, os.path.sep)    # parse the class label from the file path\n",
    "    one_hot = parts[-2] == CLASS_NAMES    # select only part with class name and create boolean array\n",
    "    label = tf.argmax(one_hot)    # get label as integer from boolean array\n",
    "    \n",
    "    return (image, label)\n",
    "\n",
    "def configure_for_performance(ds, shuffle=False):\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=2000)\n",
    "    ds = ds.batch(batch_size=BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0631f227",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af5be370fb89abbcf77a1fc46159a105",
     "grade": false,
     "grade_id": "cell-5250f5f290c08ba0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_ds = tf.data.Dataset.list_files(str(base_dir/'train/*/*.jpg'))\n",
    "val_ds   = tf.data.Dataset.list_files(str(base_dir/'validation/*/*.jpg'))\n",
    "test_ds  = tf.data.Dataset.list_files(str(base_dir/'test/*/*.jpg'))\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    # Set `num_parallel_calls` so multiple images are loaded/processed in parallel.\n",
    "    train_ds = train_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "    val_ds = val_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "    test_ds = test_ds.map(load_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    train_ds = configure_for_performance(train_ds, shuffle=True)\n",
    "    val_ds   = configure_for_performance(val_ds)\n",
    "    test_ds  = configure_for_performance(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d54759-2229-4b91-9166-1aba883f3d3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57050fd06626067d87b046e6a995ec0b",
     "grade": false,
     "grade_id": "cell-e50869f257023088",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The line `with tf.device('/cpu:0'):` indicates that the transformations of the images should be performed on the CPU and not on the GPU. This is considered a good practice to make sure that the GPU is only used for training the deep neural network model. More on that in this article: [Building a data pipeline (Stanford University)](https://cs230.stanford.edu/blog/datapipeline/#best-practices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0909dfb-74de-4f60-b452-89e073cf5ceb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3427f3c93c66487ed20127b8623861b",
     "grade": false,
     "grade_id": "cell-4ad52d9777aa69f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> CNN training without Data augmentation.</h3>\n",
    "        \n",
    "Your task is to build a Convolutional Neural Network for training on the \"cats vs dogs\" dataset. We recommend the following architecture:\n",
    "\n",
    "We will use an ANN consisting of the following layers:\n",
    " \n",
    "- 3 blocks of:\n",
    "   1. Conv2D layer with 32 units, kernel size (3,3), activation ReLU\n",
    "   2. Max pooling layer, kernel size (2,2)\n",
    "\n",
    "    \n",
    "- flattening layer\n",
    "- dense layer with 128 units and ReLU activation\n",
    "- output layer with **1 unit** and **sigmoid** activation\n",
    " \n",
    "    \n",
    "Use `train_ds` and `val_ds` for training (about 20 epochs). Save your model as 'model.h5'.\\\n",
    "Accuracy evaluated on `test_ds` should be **above 0.67**.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31699f60-e1c2-4e66-8bfa-5904adb4a353",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81c2e715741d09688205855688253df2",
     "grade": false,
     "grade_id": "cell-ac8701713241c380",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set trainig=False when validating or submitting notebook\n",
    "# and set training=True, when training network\n",
    "training=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41539e61-a633-4d33-8c06-e94363739cf2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2dfe30ece3aae12b54594d9da5db7e87",
     "grade": true,
     "grade_id": "cell-28483bf912a98d49",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this hidden cell is for setting flag training=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fd36bdc-1a2b-4e11-83e3-1320b3dbee58",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687f2220d50d94379cab49c7f2c040a5",
     "grade": false,
     "grade_id": "cell-5360a38bc7529f6a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e53c41a0e7b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m tf.keras.utils.plot_model(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \"\"\"\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2522\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "# define the model architecture using Sequential API\n",
    "# model = tf.keras.models.Sequential(...\n",
    "#        ...\n",
    "#        ...)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", name=\"cv1\"),\n",
    "    layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", name=\"cv2\"),\n",
    "    layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", name=\"cv3\"),\n",
    "    layers.MaxPooling2D(pool_size=2, name=\"maxpool\"),\n",
    "    layers.Flatten(name=\"flatten\"),\n",
    "    layers.Dense(128, activation=\"relu\", name=\"dense\"),\n",
    "    layers.Dense(1, activation='softmax', name=\"output\")\n",
    "])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model, \n",
    "    show_shapes=True, \n",
    "    show_layer_names=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70ac71",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b4759eedffceff04f3b82a5ccbf621e",
     "grade": false,
     "grade_id": "cell-bca01e5c8a1f7520",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For binary classification task we use loss `binary_crossentropy` and metrics `accuracy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2db6adfc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "133510af391dd90acd16e853c78277a4",
     "grade": false,
     "grade_id": "cell-2f7dd5a3d5cb68b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(),\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f91c165-0447-49ba-bfe5-d6298546bb10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a266d51780ac69fc17101b521b0912f",
     "grade": false,
     "grade_id": "cell-f8f9b4d4f0ebdae6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before starting training, let's recap parameter **epochs** in `.fit method()`. \n",
    "**Epochs** parameter defines the number of times the model is trained over the ENTIRE dataset. \n",
    "\n",
    "Suppose, there are 1000 training samples. We divide the training samples into 5 batches of size 200 samples. We train the model using 200 samples, i.e first batch (from 1st to 200th) from the training dataset. Next, we take the second 200 samples (from 201st to 300th) and trains the network again. We can keep doing training until we have propagated all samples in the training set through the network. When we train the model once, using the ENTIRE dataset, we call it one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e0d8e-a4bc-44c2-9d37-8a4354605caf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "746ac7737c4297d4df055e62abc43c0d",
     "grade": false,
     "grade_id": "cell-7232a69e5e5b489b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**NB!** In an earlier version of Keras, the`.fit_generator()` method is used to train the model when using a data generator as input. In the current version `.fit_generator()` function  IS DEPRECATED and will be removed in a future version.  Now, `.fit()` method supports generator as an input. \\\n",
    "To pass generator to` validation_data` argument set `validation_data = val_ds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683951d2-5c94-412f-9e1f-3b27cede069e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "061b29da180b4629f559a25860ef6d64",
     "grade": false,
     "grade_id": "cell-e3cc670d13095138",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/63 [========>.....................] - ETA: 1:14 - loss: 4.0857 - accuracy: 0.4984"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the CNN (~10 min on CPU)\n",
    "if training:\n",
    "    # history = ...\n",
    "    # model.save(\"model.h5\")\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "    history = model.fit(train_ds, batch_size=5,epochs=20,verbose=1,validation_data=val_ds)\n",
    "    model.save('model.h5')\n",
    "else:\n",
    "    model = tf.keras.models.load_model(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda64e4-4104-46d8-9845-2375849b442a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5295b94421c6bef2d3a670bcc0471d45",
     "grade": false,
     "grade_id": "cell-e9aeee5fc118d30f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot the history of the training and check accuracy on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f970c92-22c7-4881-9b34-772d1f0afa4d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "623a0614de09476190f6234010ffd0a6",
     "grade": false,
     "grade_id": "cell-dcf2970cbd438569",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # plot training log\n",
    "    if training:\n",
    "        fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "        df_accuracy = pd.DataFrame(history.history).loc[:,['accuracy','val_accuracy']]\n",
    "        df_loss = pd.DataFrame(history.history).loc[:,['loss','val_loss']]\n",
    "\n",
    "        df_accuracy.plot(ax=ax[0])\n",
    "        df_loss.plot(ax=ax[1])\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "def check_accuracy(model, expected_accuracy):\n",
    "    # check test \n",
    "    test_loss, test_acc = model.evaluate(test_ds)\n",
    "    print(f'The test set accuracy of model is {test_acc:.2f}')\n",
    "    \n",
    "    assert test_acc>expected_accuracy, \"Accuracy is too low!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89bd397-7e38-44f9-9867-1fec9d95461f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc40f9aa2901458dcda29ccb60d8336a",
     "grade": false,
     "grade_id": "cell-f4cdd89e8f0f3548",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "\n",
    "if training:\n",
    "    plot_history(history)\n",
    "\n",
    "    model = tf.keras.models.load_model(\"model.h5\")\n",
    "    check_accuracy(model, 0.67)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ada94a-0455-4e9e-b38d-2f799737e7e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "558356dc8657ba2e6608e7447639642e",
     "grade": true,
     "grade_id": "cell-eb488f79fc8dcda6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd5685f-959d-4b19-825c-fbb4a0e8cf22",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "613180b5ee5ec3b304571394d39b850c",
     "grade": false,
     "grade_id": "cell-4fecf498d146e905",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With this model, we reach an accuracy of ~ 0.70 on the test set. From the accuracy and loss plots, we can see that our model is overfitting immediately after the first epochs. \\\n",
    "If you will train the model longer (~300 epochs) you will see that the model will overfit most of the training time, and validation and test accuracy will not improve above 0.70. \n",
    "\n",
    "$ $\n",
    "<figure>\n",
    "    <img src=\"../../../coursedata/R4/bs_aug.png\" width=600/ align='middle'/>\n",
    "</figure>\n",
    "\n",
    "\n",
    "As we are training our model with a relatively small training dataset (~2000 training images), it is not surprising to see overfitting happening. Below, we will try a technique to overcome the problem of overfitting when training with a small dataset (image augmentation). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecef9209-83cc-43e9-9d27-4636eed14692",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "45d893f3c7a4431193f7df5390b6d457",
     "grade": false,
     "grade_id": "cell-752ae5bea95c0446",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## CNN training with Data Augmentation\n",
    "\n",
    "**[Data augmentation](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)** is a technique to create new training images by applying random image transformation like random rotation, shifts, shear, and flips, etc. These random image transformations can be combined to make many variants of the original images and transformed images are fed to ANN. Potentially, data augmentation can prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cb0839",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "798564c971a77b8b6db3db4f735f46ef",
     "grade": false,
     "grade_id": "cell-2ae4bd9fffd0c262",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are two ways in which you can use the data augmentation model.\n",
    "\n",
    "1. Make the preprocessing layers part of your model. In this case, the data augmentation will run on-device, synchronously with the rest of your layers, and benefit from GPU acceleration (if you have one). Additionally, exporting the model will result in saving the preprocessing layers which means that the model will automatically standardize images at a later point.\n",
    "\n",
    "2. Apply the preprocessing to your dataset, ie., using the data augmentation as the `map_func` for the [Dataset.map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) function in the training dataset. Using this approach, the data augmentation will happen asynchronously on the CPU and is non-blocking. This means that while the model is being trained with the batch `n`, Tensorflow will be preparing the batch `n+1` in parallel (you need to use [Dataset.prefetch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) to achieve this). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1becd851-831d-43b7-a02c-2cff3befa395",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e3e7a83cd4d7e77a99b29bd6ac257e3",
     "grade": false,
     "grade_id": "cell-ca4a89574babe1e4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Both methods can be implemented with Keras Tensorflow :\n",
    "\n",
    "1. Use the `Sequential` class and the preprocessing module to build a series of data augmentation operations. \n",
    "2. Apply transformations with tf.image functions and create the input data pipeline with tf.data.Dataset.\n",
    "\n",
    "**Note**, that the validation and test data should not be augmented! We augment only the images used for the model training, but not for the model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2ec79e-1246-4f8f-8dac-ae11702c9165",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d488c3c62b543edc45e84c6a2c6724ce",
     "grade": false,
     "grade_id": "cell-33b15ede92416c10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data augmentation using Keras layers \n",
    "\n",
    "In the preprocessing module, we can find many layers that can be use to apply transformations to images. For example,\n",
    "\n",
    "* [RandomCrop](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomCrop): Randomly crop the images to target height and width.\n",
    "\n",
    "* [RandomContrast](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomContrast):  Adjust the contrast of an image or images by a random factor.\n",
    "\n",
    "* [RandomFlip](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip): Randomly flip each image horizontally and vertically.\n",
    "\n",
    "* [RandomRotation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomTranslation): Randomly rotate each image.\n",
    "\n",
    "* [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling): Multiply inputs by scale and adds offset.\n",
    "\n",
    "* [Resizing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing): Image resizing layer.\n",
    "\n",
    "You can find the full list of layers in the [tf.keras.layers.experimental.preprocessing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing) module. \n",
    "\n",
    "Let's define the `Sequential` model for the data augmentation. As you can see, we are using exactly the same class that we use to create the ANN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cbc9c8-88f3-4c4d-87e6-02b041d5318f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a27514db70c2cea1ae0a544b51efb761",
     "grade": false,
     "grade_id": "cell-2ea6f67e75c72cf4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
    "\n",
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        RandomFlip(\"horizontal\", input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "        RandomRotation(0.1, fill_mode='constant'),\n",
    "        RandomZoom(0.1,0.1, fill_mode='constant')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12649cd4-04ec-4ee5-932f-c664236aa040",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98ebf1befdfec80319af9540d3f35491",
     "grade": false,
     "grade_id": "cell-21e0405a6d04f2c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Below we visualize image with applied transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdab27a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e8a4f88c3965a65ab92eb07cc8160e2",
     "grade": false,
     "grade_id": "cell-bf83fc93f83f2445",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "images, _ = train_ds.as_numpy_iterator().next()\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy())\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1894a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2feb58a05dacf32e68747000df1f8a90",
     "grade": false,
     "grade_id": "cell-630a987c671f614b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3><b>Student task.</b> CNN training with Data augmentation.</h3>\n",
    "        \n",
    "Your task is to build a Convolutional Neural Network with the following architecture:\n",
    "       \n",
    "- `data_augmentation` block defined above. You can add it to Sequential model as any other layer.\n",
    "- 3 blocks of:\n",
    "   1. Conv2D layer with 32 units, kernel size (3,3), activation ReLU\n",
    "   2. Max pooling layer, kernel size (2,2)\n",
    "\n",
    "    \n",
    "- flattening layer\n",
    "- dense layer with 128 units and ReLU activation\n",
    "- output layer with 1 unit and sigmoid activation\n",
    " \n",
    "    \n",
    "Use `train_ds` and `val_ds` for training (about 20 epochs). Save your model as 'model_aug.h5'.\\\n",
    "Accuracy evaluated on `test_ds` should be above 0.65.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ffb26",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4db6b9af542e18373acc9069533d31c1",
     "grade": false,
     "grade_id": "cell-1e55d15bb6eafc95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# define the model architecture using Sequential API\n",
    "# model_aug = tf.keras.models.Sequential(...\n",
    "#             ...\n",
    "#             ...)\n",
    "\n",
    "# model_aug.compile(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a08f0b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "226509c29b21dc496772eed41b03bc14",
     "grade": false,
     "grade_id": "cell-3b6b0473fd2b3cb1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train the CNN (~10 min on CPU)\n",
    "if training:\n",
    "    # history = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "else: \n",
    "    model_aug = tf.keras.models.load_model(\"model_aug.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcef3b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1bf4c366eb34b332e2d5d384301e0ea",
     "grade": false,
     "grade_id": "cell-2482e3c4a0a7c0a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_history(history)\n",
    "check_accuracy(model_aug, 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba72272",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31462ed5c52171c408922da30d28cbd2",
     "grade": true,
     "grade_id": "cell-a560c535ef01d31c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cb65ea-859b-4333-9a25-963d16d258d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3d4cde2614ebdf308aa3267ed6e77521",
     "grade": false,
     "grade_id": "cell-7b75e4175b9c293b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We see that data augmentation did not improve significantly the accuracy of the model predictions on the validation and test sets, but it did prevent overfitting on the training set.\n",
    "\n",
    "If you will train the model longer (~300 epochs) you will see that the model will still overfit after 50-100 epochs, but the validation and test accuracy will improve to about 0.75. Data augmentation can not fully prevent overfitting as the augmented images are still highly correlated with the original ones, thus the amount of the new information added is less than, let's say when adding new images.\n",
    "\n",
    "$ $\n",
    "<figure>\n",
    "    <img src=\"../../../coursedata/R4/keras_aug.png\" width=600/ align='middle'/>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d91f2-f0be-4681-ae97-d1a02320390c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78ed075db9037d23e28a6b1299fa68a6",
     "grade": false,
     "grade_id": "cell-cd29386d920d37db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data augmentation using tf.image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977f2bd-a6f6-4cdf-b4a2-1c7805c18b90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "958a24a8f1d0eafd502d23cb7f7b9e20",
     "grade": false,
     "grade_id": "cell-4b04f075a4dec013",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The [tf.image](https://www.tensorflow.org/api_docs/python/tf/image) module contains many functions to perform image processing. We have already seen some of them, like [tf.io.decode_jpeg](https://www.tensorflow.org/api_docs/python/tf/io/decode_jpeg) and [tf.image.resize](https://www.tensorflow.org/api_docs/python/tf/image/resize). We cannot analyze each of them in this notebook, so we encourage you to read the documentation to discover what is available to you! Let's see some of the data augmentation functions in action. But before that, we need to define some auxiliary functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8830f8e3-0ac5-45fd-8d31-01898e6cf489",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1c21f35d67a40ee83281f3a890cc6eae",
     "grade": false,
     "grade_id": "cell-250d37520d91d475",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "image_path = str(train_dir/'cats'/'cat.4.jpg')\n",
    "image, _ = load_image(image_path)\n",
    "image = image.numpy()\n",
    "\n",
    "def visualize(original, augmented):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(original)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(augmented)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2998e2-99d8-4866-af19-4a9b773023ca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0736ff554c09ad05d274d1a9262f3704",
     "grade": false,
     "grade_id": "cell-4cd4e6622b2a5936",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong>Flipping the image</strong>\n",
    "\n",
    "Flip the image either vertically or horizontally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f350bbe-2446-4d2a-b2be-d44deb31d018",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34a1bb06a1444c790e064fd5a3558963",
     "grade": false,
     "grade_id": "cell-9f3f7e585aed2787",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "flipped = tf.image.flip_left_right(image)\n",
    "visualize(image, flipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad71d9a-0134-464e-bdf7-e83558a98a26",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f4cbef55bde04f0561ee9da229c40d4",
     "grade": false,
     "grade_id": "cell-05d1d544af812240",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong>Saturate the image</strong>\n",
    "\n",
    "Saturate an image by providing a saturation factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4b07bf-24a7-48c8-b73d-364db842b8a7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "52b61c6e1492035fb4eaf9e78fa35988",
     "grade": false,
     "grade_id": "cell-a40ccdf5059f25cb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "saturated = tf.image.adjust_saturation(image, 3)\n",
    "visualize(image, saturated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a125aa-c086-4623-b5e6-e2ff84b44a6f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c9ad2b5cac2c9adad03ece0cec3492f",
     "grade": false,
     "grade_id": "cell-e3c249c931663e47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong>Center crop the image</strong>\n",
    "\n",
    "Crop the image from center up to the image part you desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b3e77-660d-4f37-95c8-df736608b846",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5cd3bb932bdfd1ff7a37245b48ff329",
     "grade": false,
     "grade_id": "cell-4aef34e666181af7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cropped = tf.image.central_crop(image, central_fraction=0.5)\n",
    "visualize(image,cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b716eaf-02b5-43e1-8d11-1c788b2346b0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5058c291da5b79c060e3d8b8313b363e",
     "grade": false,
     "grade_id": "cell-1eadf68a0cba020f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<strong>Rotate the image</strong>\n",
    "\n",
    "Rotate an image by 90 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9260ce0-ca52-479f-94a0-f756d403a2eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba587f79b18e8f90131a241bd51ab7a",
     "grade": false,
     "grade_id": "cell-024dc719198fd4d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "rotated = tf.image.rot90(image)\n",
    "visualize(image, rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b79cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6166ae13081097c8af22a2d2454dc836",
     "grade": false,
     "grade_id": "cell-b4414a7680983b47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the tutorial [Data augmentation](https://www.tensorflow.org/tutorials/images/data_augmentation) from the Tensorflow documentation you can find other examples including functions to perform random transformations similar to the Keras layers.\n",
    "\n",
    "When we use the `tf.image` module, the data augmentation is not a part of the model, thus we need to create a function that will contain all the transformations we want to apply and then pass it to the `tf.Dataset.map` function. Let's create such function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec2a02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c589942878f09eb87732f6a01b43027f",
     "grade": false,
     "grade_id": "cell-868714f5c85ebc70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def load_image_aug(image_path):\n",
    "    # load image\n",
    "    image = tf.io.read_file(image_path)    # read the image from disk\n",
    "    image = tf.io.decode_jpeg(image, channels=3)   # decode jpeg  \n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])    # resize\n",
    "    \n",
    "    # transform\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_crop(image, [120, 120, 3])\n",
    "    image = tf.image.resize_with_pad(image, 150, 150)\n",
    "    image = (image / 255.0)    # scale \n",
    "    \n",
    "    # get lable value from path\n",
    "    parts = tf.strings.split(image_path, os.path.sep)    # parse the class label from the file path\n",
    "    one_hot = parts[-2] == CLASS_NAMES    # select only part with class name and create boolean array\n",
    "    label = tf.argmax(one_hot)    # get label as integer from boolean array\n",
    "    \n",
    "    return (image, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55f6655",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "674307f55a0db6f31ab61f4c4e492283",
     "grade": false,
     "grade_id": "cell-626f55cb4531cf05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We only need to create a new generator for training dataset, as validation and test sets should not be augmented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efdb2e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d36b3f8407c6dc71d4f54d0586328e6a",
     "grade": false,
     "grade_id": "cell-e05a929254500aa6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.list_files(str(base_dir/'train/*/*.jpg'))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    train_ds = train_ds.map(load_image_aug, num_parallel_calls=AUTOTUNE)\n",
    "    train_ds = configure_for_performance(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea44502",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f26c7da554f88e97053b4b57bf51d3f1",
     "grade": false,
     "grade_id": "cell-024ed79da0824463",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy())\n",
    "        plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5e5a34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "04f7b6082b0833cf6864808ff4e1d319",
     "grade": false,
     "grade_id": "cell-9efa5e3a232629fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now you can try out `train_ds` generator and train CNN with a similar architecture as in the task \"Student task. CNN training without Data augmentation\". \\\n",
    "Note, that as augmentation is performed directly by generator, we do not add `data_augmentation` block in CNN model.\n",
    "\n",
    "After the training you will get plot similar to this one:\n",
    "\n",
    "$ $\n",
    "<figure>\n",
    "    <img src=\"../../../coursedata/R4/tfdata_aug.png\" width=600/ align='middle'/>\n",
    "</figure>\n",
    "\n",
    "As you can see, this method is a bit more involved and it requires you to implement the data augmentation pipeline by hand, but the benefit is that you gain more fine-grained control. In most of applications, using the Keras' preprocessing layers is sufficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb0db49-9664-4115-86c1-df69b80a9612",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "436122e8c5aa36a16267cbecd38e6b55",
     "grade": false,
     "grade_id": "cell-b7f73d8b45a497d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we've shown how to create a tf.Dataset and use it for training and validation of the CNN model. The tf.Dataset creates small batches of data to sequentially feed these batches to the neural networks and train the model. They are an effective way to train the model when you have a large dataset and limited computation power. We covered the different ways you can define the data augmentation proccess and we discuss the pros and cons of each approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5836fcf-0beb-4670-b7d1-866878094c92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d16afc3e482788a539381e1bfebba710",
     "grade": false,
     "grade_id": "cell-d750ee5844790340",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce45653-3e2d-45c7-89c5-b9e894ad5771",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d82fab248d7d1b2e0f2f1c533b987e99",
     "grade": false,
     "grade_id": "cell-7b6006e446c73e7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 1.</h3>\n",
    "        \n",
    "Choose the correct statement:\n",
    "\n",
    "1. With limited RAM it is better to feed large datasets to ANN at once (not in batches)\n",
    "\n",
    "2. With limited RAM it is better to feed large datasets to ANN in batches\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadac034-5465-41ed-bab0-70e90d175ec2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a183a47a75128ba76d127251a775fd0",
     "grade": false,
     "grade_id": "cell-1875da9f42208abd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa1419-297e-422f-be90-a6f474a3b45f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f7ed3e27c650324a50050ff6e0a98b6",
     "grade": true,
     "grade_id": "cell-6b844bcfe8f1fb18",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_1 in [1,2], '\"answer\" Value should be an integer between 1 and 2.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eceac2-6535-49f3-9997-2800daffe196",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3271aceb6c480533c1bf4d164d76dfff",
     "grade": false,
     "grade_id": "cell-0372d0e0676f1a59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 2.</h3>\n",
    "    \n",
    "Choose the correct statement:\n",
    "\n",
    "1. Large training dataset will lead to overfitting\n",
    "\n",
    "2. Small training dataset will lead to overfitting\n",
    "\n",
    "3. Large training dataset will lead to worse model generalization\n",
    "\n",
    "4. Model generalization is better when using small training dataset\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4099299-6dc2-4287-91b2-15863036b814",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbf708a2cb88cfba87215ba4b7e17b1a",
     "grade": false,
     "grade_id": "cell-89977dc29df46336",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_2 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c268c-bd17-4202-a9c1-39200f505cc6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6bf76bee5081532ba4340a753fe917bc",
     "grade": true,
     "grade_id": "cell-4b52bf3edc71704e",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_2 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7765ac76-5f49-4e39-a4c8-9a9ac3dfba20",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f922e1e4355801e7290b3dc6ab841055",
     "grade": false,
     "grade_id": "cell-16c0657e3cdb76fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 3.</h3>\n",
    "    \n",
    "Choose the correct statement. Image augmentation is:\n",
    "\n",
    "1. Generating completely new images, not correlated with the original images\n",
    "\n",
    "2. Used to improve fitting of the training data\n",
    "\n",
    "3. Improving the colour and brightness of the image\n",
    "\n",
    "4. Applying random transformations to the images, such as rotation, flipping\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e4de8d-cc12-41df-a5c2-7aa468e9b696",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30147aa7cf1ee26ff239ccc5e288b695",
     "grade": false,
     "grade_id": "cell-e537b313d4e82982",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_3 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929dbe1-0bb3-4459-975f-685e7536d603",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2e82a42ec04eb02b06027e23782fa2e",
     "grade": true,
     "grade_id": "cell-42bc558f39d19a08",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_3 in [1,2,3,4], '\"answer\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cc5ea-2318-4a3a-9559-6731745eff9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cca4b00bf62ee496db2deb06850b31b3",
     "grade": false,
     "grade_id": "cell-1c7b31ca1a90b697",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "    <h3>Question 4.</h3>\n",
    "    \n",
    "Choose the correct statement. Image augmentation does:\n",
    "\n",
    "1. Does both\n",
    "\n",
    "2. Adds new images (not correlated with the original images) to the training set\n",
    "\n",
    "3. Replaces original training images with augmented ones\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed98e0c-15b9-42a5-9b13-3b8d066456c8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ec5b913cba135d0f6d5bd8701fc38fc",
     "grade": false,
     "grade_id": "cell-9f77de3958786c59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# answer_4 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448802e5-f634-424f-94e3-6e39f941742b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71e74bcd9a7f1b65d983bd6ace8767d9",
     "grade": true,
     "grade_id": "cell-b682f9ddfa3fd8e0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell is for tests\n",
    "assert answer_4 in [1,2,3], '\"answer\" Value should be an integer between 1 and 3.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "372.7916564941406px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
